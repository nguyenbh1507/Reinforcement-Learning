{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning with Tensorflow: Deep Q-Networks andÂ Beyond\n",
    "\n",
    "This iPython notebook implements a Deep Q-Network using both Double DQN and Dueling DQN. The agent learn to solve a navigation task in a basic grid world. To learn more, read here: https://medium.com/p/8438a3e2b8df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hung/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the game environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to adjust the size of the gridworld. Making it smaller provides an easier task for our DQN agent, while making the world larger increases the challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f62e10f62e8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f62e4722b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gridworld import gameEnv\n",
    "\n",
    "env = gameEnv(partial=False,size=5)\n",
    "env.reset()\n",
    "plt.imshow(env.renderEnv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is an example of a starting environment in our simple game. The agent controls the blue square, and can move up, down, left, or right. The goal is to move to the green square (for +1 reward) and avoid the red square (for -1 reward). The position of the three blocks is randomized every episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the network itself\n",
    "\n",
    "Remember that:\n",
    "\n",
    "**Double DQNs**\n",
    "$$\n",
    "\\underbrace{\\mathcal{Q}(s,a)}_{\\text{TD target}}=r(s,a) +\\underbrace{\\gamma \\mathcal{Q}\\left(s',\\underbrace{arg\\max_a\\mathcal{Q}(s',a)}_{\\begin{matrix}\\text{DQN chooses action}\\\\\\text{for the next state}\\end{matrix}}\\right)}_{\\begin{matrix}\\text{target network calculates the Q-}\\\\ \\text{value of taking that action at state }s\\end{matrix}}\\tag{1}\n",
    "$$\n",
    "\n",
    "**Dueling DQNs**\n",
    "$$\n",
    "\\mathcal{Q}(s,a;\\theta,\\alpha,\\beta)=V(s;\\theta,\\beta)+\\left[A(s,a;\\theta,\\alpha)-\\underbrace{\\frac{1}{\\mathcal{A}}\\sum_{a'}A(s,a';\\theta,\\alpha)}_{\\text{average advantage}}\\right]\n",
    "\\tag{2}$$\n",
    "where:\n",
    "- $\\theta$ - common network parameters\n",
    "- $\\alpha$ - advantage stream parameters\n",
    "- $\\beta$ - value stream parameters\n",
    "\n",
    "**DQN architecture:**\n",
    "![](https://cdn-images-1.medium.com/max/873/1*N_t9I7MeejAoWlDuH1i7cw.png)\n",
    "Above: Regular DQN with a single stream for Q-values. Below: Dueling DQN where the value and advantage are calculated separately and then combined only at the final layer into a Q value.\n",
    "\n",
    "**Loss**\n",
    "$$\n",
    "loss = \\left[\\underbrace{r\\quad+\\quad\\gamma\\max_{a'}\\hat{\\mathcal{Q}}(s,a')}_{\\text{Target}} \\quad-\\quad \\underbrace{\\mathcal{Q}(s,a)}_{\\text{Prediction}}\\right]^2 \\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self, h_size, name='DQN'):\n",
    "        \"\"\"\n",
    "        The network recieves a frame from the game, flattened into an array.\n",
    "        It then resizes it and processes it through four convolutional layers.\n",
    "        \"\"\"\n",
    "        # Scalar input, from images size 84x84x3\n",
    "        self.scalarInput =  tf.placeholder(shape=[None,21168], dtype=tf.float32)  # 21168 = 84x84x3\n",
    "        \n",
    "        # Conver scalar input to images\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,3])\n",
    "        \n",
    "        # First convNet\n",
    "        # In 84x84x3, out 20x20x32\n",
    "        self.conv1 = \\\n",
    "            slim.conv2d(\n",
    "                inputs=self.imageIn,\n",
    "                num_outputs=32,\n",
    "                kernel_size=[8,8],\n",
    "                stride=[4,4],\n",
    "                padding='VALID',\n",
    "                weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                biases_initializer=None,\n",
    "        )\n",
    "        \n",
    "        # Second convNet\n",
    "        # In 20x20x32, out 9x9x64\n",
    "        self.conv2 = \\\n",
    "            slim.conv2d(\n",
    "                inputs=self.conv1,\n",
    "                num_outputs=64,\n",
    "                kernel_size=[4,4],\n",
    "                stride=[2,2],\n",
    "                padding='VALID',\n",
    "                weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                biases_initializer=None,\n",
    "        )\n",
    "        \n",
    "        # Third convNet\n",
    "        # in 9x9x64, out 7x7x64\n",
    "        self.conv3 = \\\n",
    "            slim.conv2d(\n",
    "                inputs=self.conv2,\n",
    "                num_outputs=64,\n",
    "                kernel_size=[3,3],\n",
    "                stride=[1,1],\n",
    "                padding='VALID', \n",
    "                biases_initializer=None,\n",
    "        )\n",
    "        \n",
    "        # Fourth convnet\n",
    "        # in 7x7x64, out 1x1xh_size\n",
    "        self.conv4 = \\\n",
    "            slim.conv2d(\n",
    "                inputs=self.conv3,\n",
    "                num_outputs=h_size,\n",
    "                kernel_size=[7,7],\n",
    "                stride=[1,1],\n",
    "                padding='VALID', \n",
    "                biases_initializer=None,\n",
    "        )\n",
    "        \n",
    "        # We take the output from the final convolutional layer \n",
    "        # and split it into separate advantage and value streams.\n",
    "        \n",
    "        self.streamAC, self.streamVC = tf.split(\n",
    "            value=self.conv4, \n",
    "            num_or_size_splits=2,\n",
    "            axis=3,\n",
    "            name=name+'_split'\n",
    "        )\n",
    "        \n",
    "        # conver to flatten\n",
    "        self.streamA = slim.flatten(self.streamAC)\n",
    "        self.streamV = slim.flatten(self.streamVC)\n",
    "        \n",
    "        xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "        self.AW = tf.Variable(xavier_init([h_size//2,env.actions]), name=name+'_AW')\n",
    "        self.VW = tf.Variable(xavier_init([h_size//2,1]), name=name+'_VW')\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW, name=name+\"_advantage\")\n",
    "        self.Value = tf.matmul(self.streamV,self.VW, name=name+'_value')\n",
    "        \n",
    "        # Then combine them together to get our final Q-values.\n",
    "        # See the formula (2)\n",
    "        self.Qout = self.Value + \\\n",
    "                    tf.subtract(\n",
    "                        self.Advantage,\n",
    "                        tf.reduce_mean(self.Advantage,axis=1,keep_dims=True)\n",
    "        )\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        # Below we obtain the loss by taking the sum of squares difference \n",
    "        # between the target and prediction Q values.\n",
    "        \n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32, name=name+'_target')\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32, name=name+'_actions')\n",
    "        self.actions_onehot = tf.one_hot(indices=self.actions, depth=env.actions, dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "        # Set objective function for training\n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss,name=name+'_updateNode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Qnetwork object at 0x7f62db7bb2b0>\n"
     ]
    }
   ],
   "source": [
    "network = Qnetwork(16)\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ArgMax_2:0' shape=(?,) dtype=int64>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class allows us to store experies and sample then randomly to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    \"\"\"\n",
    "    This class allows us to store experiences and sample then randomly to train the network.\n",
    "    experiment = (current state, action, reward, next state, over game status)\n",
    "    \"\"\"\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self, experience):\n",
    "        \"\"\"\n",
    "        Extend buffer by experience, keeping buffer size\n",
    "        \"\"\"\n",
    "        # check to remove fist elements of buffer\n",
    "        # then extend buffer by experience, keep buffer_size = constant \n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "            \n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        \"\"\"\n",
    "        Randomly take size experiences from buffer\n",
    "        \"\"\"\n",
    "        return np.reshape(\n",
    "                    np.array(random.sample(self.buffer,size)),\n",
    "                    [size,5]  # 5 = len (experiment)\n",
    "               )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple function to resize our game frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processState(states):\n",
    "    \"\"\"\n",
    "    This is a simple function to resize our game frames from 84x84x3 to 21168\n",
    "    \"\"\"\n",
    "    return np.reshape(states, [21168])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions allow us to update the parameters of our target network with those of the primary network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateTargetGraph(tfVars, tau):\n",
    "    \"\"\"\n",
    "    Create computational graph for updating parameters\n",
    "    tfVars: all variables to update\n",
    "    In this work:\n",
    "    tfVars= [\n",
    "         \n",
    "         # Primary network\n",
    "         <tf.Variable 'Conv/weights:0' shape=(8, 8, 3, 32) dtype=float32_ref>,     # convNet 1 kernel weights\n",
    "         <tf.Variable 'Conv_1/weights:0' shape=(4, 4, 32, 64) dtype=float32_ref>,  # convNet 2 kernel weights\n",
    "         <tf.Variable 'Conv_2/weights:0' shape=(3, 3, 64, 64) dtype=float32_ref>,  # convNet 3 kernel weights\n",
    "         <tf.Variable 'Conv_3/weights:0' shape=(7, 7, 64, 512) dtype=float32_ref>, # convNet 4 kernel weights\n",
    "         <tf.Variable 'mainDQN_AW:0' shape=(256, 4) dtype=float32_ref>,            # weight mattrix AW\n",
    "         <tf.Variable 'mainDQN_VW:0' shape=(256, 1) dtype=float32_ref>,            # weight mattrix VW\n",
    "         \n",
    "         # Target network\n",
    "         <tf.Variable 'Conv_4/weights:0' shape=(8, 8, 3, 32) dtype=float32_ref>,\n",
    "         <tf.Variable 'Conv_5/weights:0' shape=(4, 4, 32, 64) dtype=float32_ref>,\n",
    "         <tf.Variable 'Conv_6/weights:0' shape=(3, 3, 64, 64) dtype=float32_ref>,\n",
    "         <tf.Variable 'Conv_7/weights:0' shape=(7, 7, 64, 512) dtype=float32_ref>,\n",
    "         <tf.Variable 'targetDQN_AW:0' shape=(256, 4) dtype=float32_ref>,\n",
    "         <tf.Variable 'targetDQN_VW:0' shape=(256, 1) dtype=float32_ref>\n",
    "    ]\n",
    "        \n",
    "    tau: Rate to update target network (tfVars[total_vars//2:]) \n",
    "         toward primary network (var:= tfVars[0:total_vars//2])\n",
    "    \"\"\"\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []  # operation holder\n",
    "    \n",
    "    # Dueling process\n",
    "    # Q-vals = Q-vals + alpha * (Q-target - Q-vals)\n",
    "    #        ~ Q-target*tau + (1-tau)*Q-vals\n",
    "    # then assign W to W*\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(\n",
    "            tfVars[total_vars//2 + idx].assign(\n",
    "                (var.value()*tau) + ((1-tau)*tfVars[total_vars//2 + idx].value())\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    return op_holder\n",
    "\n",
    "\"\"\"\n",
    "In this work, \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    \"\"\"\n",
    "    Run updating operations\n",
    "    \"\"\"\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting all the training parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32          # How many experiences to use for each training step.\n",
    "update_freq = 4          # How often to perform a training step.\n",
    "y = .99                  # Discount factor on the target Q-values (gamma)\n",
    "startE = 1               # Starting chance of random action (initial exploration rate)\n",
    "endE = 0.1               # Final chance of random action (final exploration rate)\n",
    "annealing_steps = 10000. # How many steps of training to reduce startE to endE.\n",
    "num_episodes = 10000     # How many episodes of game environment to train network with.\n",
    "pre_train_steps = 10000  # How many steps of random actions before training begins.\n",
    "max_epLength = 50        # The max allowed length of our episode.\n",
    "load_model = False       # Whether to load a saved model.\n",
    "path = \"./dqn\"           # The path to save our model to.\n",
    "h_size = 512             # The size of the final convolutional layer before splitting it into \n",
    "                         # Advantage and Value streams.\n",
    "tau = 0.001              # Rate to update target network toward primary network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size, 'mainDQN')\n",
    "targetQN = Qnetwork(h_size, 'targetDQN')\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "# Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/annealing_steps\n",
    "\n",
    "# create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "# Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computational graph**\n",
    "![](https://raw.githubusercontent.com/nguyenbh1507/Example_notebooks/master/RL/Double-Dueling-DQN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model at episode number: 0\n",
      "Total steps: 12616 Mean reward for last 10 episodes: 2.5 Epsilon: 0.7645599999999098\n",
      "Total steps: 13116 Mean reward for last 10 episodes: 1.7 Epsilon: 0.7195599999998925\n",
      "Total steps: 13616 Mean reward for last 10 episodes: 1.1 Epsilon: 0.6745599999998753\n",
      "Total steps: 14116 Mean reward for last 10 episodes: 1.7 Epsilon: 0.629559999999858\n",
      "Total steps: 14616 Mean reward for last 10 episodes: 0.7 Epsilon: 0.5845599999998408\n",
      "Total steps: 15116 Mean reward for last 10 episodes: 1.5 Epsilon: 0.5395599999998235\n",
      "Total steps: 15616 Mean reward for last 10 episodes: 2.7 Epsilon: 0.49455999999980965\n",
      "Total steps: 16116 Mean reward for last 10 episodes: 2.9 Epsilon: 0.44955999999982016\n",
      "Total steps: 16616 Mean reward for last 10 episodes: 2.2 Epsilon: 0.40455999999983067\n",
      "Total steps: 17116 Mean reward for last 10 episodes: 3.4 Epsilon: 0.3595599999998412\n",
      "Total steps: 17616 Mean reward for last 10 episodes: 2.5 Epsilon: 0.3145599999998517\n",
      "Total steps: 18116 Mean reward for last 10 episodes: 1.7 Epsilon: 0.2695599999998622\n",
      "Total steps: 18616 Mean reward for last 10 episodes: 1.7 Epsilon: 0.22455999999986484\n",
      "Total steps: 19116 Mean reward for last 10 episodes: 2.1 Epsilon: 0.17955999999986147\n",
      "Total steps: 19616 Mean reward for last 10 episodes: 2.8 Epsilon: 0.1345599999998581\n",
      "Total steps: 20116 Mean reward for last 10 episodes: 2.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 20616 Mean reward for last 10 episodes: 1.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 21116 Mean reward for last 10 episodes: 2.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 21616 Mean reward for last 10 episodes: 1.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 22116 Mean reward for last 10 episodes: 3.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 22616 Mean reward for last 10 episodes: 2.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 23116 Mean reward for last 10 episodes: 3.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 23616 Mean reward for last 10 episodes: 3.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 24116 Mean reward for last 10 episodes: 3.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 24616 Mean reward for last 10 episodes: 3.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 25116 Mean reward for last 10 episodes: 3.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 25616 Mean reward for last 10 episodes: 3.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 26116 Mean reward for last 10 episodes: 5.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 26616 Mean reward for last 10 episodes: 6.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 27116 Mean reward for last 10 episodes: 5.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 27616 Mean reward for last 10 episodes: 7.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 28116 Mean reward for last 10 episodes: 6.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 28616 Mean reward for last 10 episodes: 7.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 29116 Mean reward for last 10 episodes: 10.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 29616 Mean reward for last 10 episodes: 6.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 30116 Mean reward for last 10 episodes: 7.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 30616 Mean reward for last 10 episodes: 11.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 31116 Mean reward for last 10 episodes: 10.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 31616 Mean reward for last 10 episodes: 6.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 32116 Mean reward for last 10 episodes: 8.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 32616 Mean reward for last 10 episodes: 12.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 33116 Mean reward for last 10 episodes: 10.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 33616 Mean reward for last 10 episodes: 9.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 34116 Mean reward for last 10 episodes: 14.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 34616 Mean reward for last 10 episodes: 11.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 35116 Mean reward for last 10 episodes: 12.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 35616 Mean reward for last 10 episodes: 13.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 36116 Mean reward for last 10 episodes: 15.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 36616 Mean reward for last 10 episodes: 9.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 37116 Mean reward for last 10 episodes: 11.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 37616 Mean reward for last 10 episodes: 13.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 38116 Mean reward for last 10 episodes: 15.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 38616 Mean reward for last 10 episodes: 16.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 39116 Mean reward for last 10 episodes: 14.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 39616 Mean reward for last 10 episodes: 19.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 40116 Mean reward for last 10 episodes: 15.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 40616 Mean reward for last 10 episodes: 15.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 41116 Mean reward for last 10 episodes: 15.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 41616 Mean reward for last 10 episodes: 19.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 42116 Mean reward for last 10 episodes: 17.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 42616 Mean reward for last 10 episodes: 16.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 43116 Mean reward for last 10 episodes: 17.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 43616 Mean reward for last 10 episodes: 16.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 44116 Mean reward for last 10 episodes: 20.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 44616 Mean reward for last 10 episodes: 18.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 45116 Mean reward for last 10 episodes: 19.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 45616 Mean reward for last 10 episodes: 13.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 46116 Mean reward for last 10 episodes: 16.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 46616 Mean reward for last 10 episodes: 19.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 47116 Mean reward for last 10 episodes: 18.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 47616 Mean reward for last 10 episodes: 16.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 48116 Mean reward for last 10 episodes: 20.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 48616 Mean reward for last 10 episodes: 19.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 49116 Mean reward for last 10 episodes: 22.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 49616 Mean reward for last 10 episodes: 16.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 50116 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 50616 Mean reward for last 10 episodes: 19.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 51116 Mean reward for last 10 episodes: 20.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 51616 Mean reward for last 10 episodes: 20.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 52116 Mean reward for last 10 episodes: 20.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 52616 Mean reward for last 10 episodes: 21.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 53116 Mean reward for last 10 episodes: 21.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 53616 Mean reward for last 10 episodes: 22.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 54116 Mean reward for last 10 episodes: 17.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 54616 Mean reward for last 10 episodes: 18.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 55116 Mean reward for last 10 episodes: 20.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 55616 Mean reward for last 10 episodes: 22.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 56116 Mean reward for last 10 episodes: 21.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 56616 Mean reward for last 10 episodes: 20.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 57116 Mean reward for last 10 episodes: 18.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 57616 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 58116 Mean reward for last 10 episodes: 21.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 58616 Mean reward for last 10 episodes: 19.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 59116 Mean reward for last 10 episodes: 21.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 59616 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 60116 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 60616 Mean reward for last 10 episodes: 21.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 61116 Mean reward for last 10 episodes: 20.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 61616 Mean reward for last 10 episodes: 21.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 62116 Mean reward for last 10 episodes: 21.5 Epsilon: 0.09999999999985551\n",
      "Saved model at episode number: 1000\n",
      "Total steps: 62616 Mean reward for last 10 episodes: 21.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 63116 Mean reward for last 10 episodes: 19.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 63616 Mean reward for last 10 episodes: 20.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 64116 Mean reward for last 10 episodes: 21.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 64616 Mean reward for last 10 episodes: 22.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 65116 Mean reward for last 10 episodes: 23.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 65616 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 66116 Mean reward for last 10 episodes: 18.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 66616 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 67116 Mean reward for last 10 episodes: 21.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 67616 Mean reward for last 10 episodes: 21.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 68116 Mean reward for last 10 episodes: 23.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 68616 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 69116 Mean reward for last 10 episodes: 22.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 69616 Mean reward for last 10 episodes: 23.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 70116 Mean reward for last 10 episodes: 21.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 70616 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 71116 Mean reward for last 10 episodes: 22.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 71616 Mean reward for last 10 episodes: 22.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 72116 Mean reward for last 10 episodes: 21.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 72616 Mean reward for last 10 episodes: 21.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 73116 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 73616 Mean reward for last 10 episodes: 19.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 74116 Mean reward for last 10 episodes: 19.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 74616 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 75116 Mean reward for last 10 episodes: 20.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 75616 Mean reward for last 10 episodes: 22.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 76116 Mean reward for last 10 episodes: 21.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 76616 Mean reward for last 10 episodes: 21.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 77116 Mean reward for last 10 episodes: 23.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 77616 Mean reward for last 10 episodes: 21.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 78116 Mean reward for last 10 episodes: 19.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 78616 Mean reward for last 10 episodes: 21.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 79116 Mean reward for last 10 episodes: 24.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 79616 Mean reward for last 10 episodes: 21.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 80116 Mean reward for last 10 episodes: 20.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 80616 Mean reward for last 10 episodes: 22.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 81116 Mean reward for last 10 episodes: 21.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 81616 Mean reward for last 10 episodes: 21.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 82116 Mean reward for last 10 episodes: 22.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 82616 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 83116 Mean reward for last 10 episodes: 22.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 83616 Mean reward for last 10 episodes: 22.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 84116 Mean reward for last 10 episodes: 19.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 84616 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 85116 Mean reward for last 10 episodes: 20.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 85616 Mean reward for last 10 episodes: 21.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 86116 Mean reward for last 10 episodes: 21.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 86616 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 87116 Mean reward for last 10 episodes: 19.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 87616 Mean reward for last 10 episodes: 21.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 88116 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 88616 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 89116 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 89616 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 90116 Mean reward for last 10 episodes: 20.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 90616 Mean reward for last 10 episodes: 21.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 91116 Mean reward for last 10 episodes: 19.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 91616 Mean reward for last 10 episodes: 20.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 92116 Mean reward for last 10 episodes: 22.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 92616 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 93116 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 93616 Mean reward for last 10 episodes: 20.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 94116 Mean reward for last 10 episodes: 17.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 94616 Mean reward for last 10 episodes: 19.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 95116 Mean reward for last 10 episodes: 21.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 95616 Mean reward for last 10 episodes: 24.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 96116 Mean reward for last 10 episodes: 22.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 96616 Mean reward for last 10 episodes: 22.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 97116 Mean reward for last 10 episodes: 21.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 97616 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 98116 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 98616 Mean reward for last 10 episodes: 22.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 99116 Mean reward for last 10 episodes: 22.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 99616 Mean reward for last 10 episodes: 17.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 100116 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 100616 Mean reward for last 10 episodes: 20.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 101116 Mean reward for last 10 episodes: 23.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 101616 Mean reward for last 10 episodes: 21.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 102116 Mean reward for last 10 episodes: 23.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 102616 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 103116 Mean reward for last 10 episodes: 24.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 103616 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 104116 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 104616 Mean reward for last 10 episodes: 23.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 105116 Mean reward for last 10 episodes: 19.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 105616 Mean reward for last 10 episodes: 23.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 106116 Mean reward for last 10 episodes: 21.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 106616 Mean reward for last 10 episodes: 21.8 Epsilon: 0.09999999999985551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 107116 Mean reward for last 10 episodes: 23.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 107616 Mean reward for last 10 episodes: 20.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 108116 Mean reward for last 10 episodes: 20.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 108616 Mean reward for last 10 episodes: 20.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 109116 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 109616 Mean reward for last 10 episodes: 20.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 110116 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 110616 Mean reward for last 10 episodes: 23.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 111116 Mean reward for last 10 episodes: 20.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 111616 Mean reward for last 10 episodes: 21.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 112116 Mean reward for last 10 episodes: 19.1 Epsilon: 0.09999999999985551\n",
      "Saved model at episode number: 2000\n",
      "Total steps: 112616 Mean reward for last 10 episodes: 21.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 113116 Mean reward for last 10 episodes: 19.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 113616 Mean reward for last 10 episodes: 23.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 114116 Mean reward for last 10 episodes: 20.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 114616 Mean reward for last 10 episodes: 21.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 115116 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 115616 Mean reward for last 10 episodes: 22.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 116116 Mean reward for last 10 episodes: 23.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 116616 Mean reward for last 10 episodes: 22.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 117116 Mean reward for last 10 episodes: 21.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 117616 Mean reward for last 10 episodes: 23.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 118116 Mean reward for last 10 episodes: 23.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 118616 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 119116 Mean reward for last 10 episodes: 21.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 119616 Mean reward for last 10 episodes: 22.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 120116 Mean reward for last 10 episodes: 20.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 120616 Mean reward for last 10 episodes: 23.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 121116 Mean reward for last 10 episodes: 23.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 121616 Mean reward for last 10 episodes: 21.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 122116 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 122616 Mean reward for last 10 episodes: 22.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 123116 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 123616 Mean reward for last 10 episodes: 21.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 124116 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 124616 Mean reward for last 10 episodes: 20.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 125116 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 125616 Mean reward for last 10 episodes: 23.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 126116 Mean reward for last 10 episodes: 22.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 126616 Mean reward for last 10 episodes: 20.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 127116 Mean reward for last 10 episodes: 22.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 127616 Mean reward for last 10 episodes: 20.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 128116 Mean reward for last 10 episodes: 24.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 128616 Mean reward for last 10 episodes: 21.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 129116 Mean reward for last 10 episodes: 21.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 129616 Mean reward for last 10 episodes: 20.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 130116 Mean reward for last 10 episodes: 22.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 130616 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 131116 Mean reward for last 10 episodes: 21.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 131616 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 132116 Mean reward for last 10 episodes: 22.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 132616 Mean reward for last 10 episodes: 24.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 133116 Mean reward for last 10 episodes: 23.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 133616 Mean reward for last 10 episodes: 23.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 134116 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 134616 Mean reward for last 10 episodes: 23.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 135116 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 135616 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 136116 Mean reward for last 10 episodes: 22.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 136616 Mean reward for last 10 episodes: 23.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 137116 Mean reward for last 10 episodes: 23.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 137616 Mean reward for last 10 episodes: 22.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 138116 Mean reward for last 10 episodes: 23.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 138616 Mean reward for last 10 episodes: 22.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 139116 Mean reward for last 10 episodes: 21.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 139616 Mean reward for last 10 episodes: 20.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 140116 Mean reward for last 10 episodes: 21.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 140616 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 141116 Mean reward for last 10 episodes: 21.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 141616 Mean reward for last 10 episodes: 21.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 142116 Mean reward for last 10 episodes: 20.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 142616 Mean reward for last 10 episodes: 24.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 143116 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 143616 Mean reward for last 10 episodes: 20.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 144116 Mean reward for last 10 episodes: 19.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 144616 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 145116 Mean reward for last 10 episodes: 21.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 145616 Mean reward for last 10 episodes: 21.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 146116 Mean reward for last 10 episodes: 21.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 146616 Mean reward for last 10 episodes: 23.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 147116 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 147616 Mean reward for last 10 episodes: 20.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 148116 Mean reward for last 10 episodes: 23.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 148616 Mean reward for last 10 episodes: 22.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 149116 Mean reward for last 10 episodes: 23.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 149616 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 150116 Mean reward for last 10 episodes: 23.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 150616 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 151116 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 151616 Mean reward for last 10 episodes: 22.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 152116 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 152616 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 153116 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 153616 Mean reward for last 10 episodes: 20.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 154116 Mean reward for last 10 episodes: 20.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 154616 Mean reward for last 10 episodes: 23.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 155116 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 155616 Mean reward for last 10 episodes: 21.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 156116 Mean reward for last 10 episodes: 21.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 156616 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 157116 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 157616 Mean reward for last 10 episodes: 21.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 158116 Mean reward for last 10 episodes: 22.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 158616 Mean reward for last 10 episodes: 20.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 159116 Mean reward for last 10 episodes: 22.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 159616 Mean reward for last 10 episodes: 21.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 160116 Mean reward for last 10 episodes: 23.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 160616 Mean reward for last 10 episodes: 23.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 161116 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 161616 Mean reward for last 10 episodes: 23.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 162116 Mean reward for last 10 episodes: 23.5 Epsilon: 0.09999999999985551\n",
      "Saved model at episode number: 3000\n",
      "Total steps: 162616 Mean reward for last 10 episodes: 24.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 163116 Mean reward for last 10 episodes: 22.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 163616 Mean reward for last 10 episodes: 21.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 164116 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 164616 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 165116 Mean reward for last 10 episodes: 23.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 165616 Mean reward for last 10 episodes: 22.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 166116 Mean reward for last 10 episodes: 23.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 166616 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 167116 Mean reward for last 10 episodes: 21.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 167616 Mean reward for last 10 episodes: 23.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 168116 Mean reward for last 10 episodes: 22.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 168616 Mean reward for last 10 episodes: 20.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 169116 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 169616 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 170116 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 170616 Mean reward for last 10 episodes: 24.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 171116 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 171616 Mean reward for last 10 episodes: 22.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 172116 Mean reward for last 10 episodes: 21.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 172616 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 173116 Mean reward for last 10 episodes: 23.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 173616 Mean reward for last 10 episodes: 23.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 174116 Mean reward for last 10 episodes: 21.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 174616 Mean reward for last 10 episodes: 20.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 175116 Mean reward for last 10 episodes: 23.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 175616 Mean reward for last 10 episodes: 21.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 176116 Mean reward for last 10 episodes: 24.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 176616 Mean reward for last 10 episodes: 22.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 177116 Mean reward for last 10 episodes: 22.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 177616 Mean reward for last 10 episodes: 20.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 178116 Mean reward for last 10 episodes: 21.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 178616 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 179116 Mean reward for last 10 episodes: 21.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 179616 Mean reward for last 10 episodes: 19.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 180116 Mean reward for last 10 episodes: 21.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 180616 Mean reward for last 10 episodes: 23.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 181116 Mean reward for last 10 episodes: 23.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 181616 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 182116 Mean reward for last 10 episodes: 20.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 182616 Mean reward for last 10 episodes: 19.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 183116 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 183616 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 184116 Mean reward for last 10 episodes: 24.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 184616 Mean reward for last 10 episodes: 21.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 185116 Mean reward for last 10 episodes: 21.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 185616 Mean reward for last 10 episodes: 20.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 186116 Mean reward for last 10 episodes: 24.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 186616 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 187116 Mean reward for last 10 episodes: 24.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 187616 Mean reward for last 10 episodes: 20.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 188116 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 188616 Mean reward for last 10 episodes: 23.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 189116 Mean reward for last 10 episodes: 21.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 189616 Mean reward for last 10 episodes: 22.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 190116 Mean reward for last 10 episodes: 21.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 190616 Mean reward for last 10 episodes: 21.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 191116 Mean reward for last 10 episodes: 24.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 191616 Mean reward for last 10 episodes: 21.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 192116 Mean reward for last 10 episodes: 22.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 192616 Mean reward for last 10 episodes: 22.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 193116 Mean reward for last 10 episodes: 24.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 193616 Mean reward for last 10 episodes: 23.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 194116 Mean reward for last 10 episodes: 19.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 194616 Mean reward for last 10 episodes: 23.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 195116 Mean reward for last 10 episodes: 20.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 195616 Mean reward for last 10 episodes: 19.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 196116 Mean reward for last 10 episodes: 24.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 196616 Mean reward for last 10 episodes: 23.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 197116 Mean reward for last 10 episodes: 22.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 197616 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 198116 Mean reward for last 10 episodes: 21.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 198616 Mean reward for last 10 episodes: 22.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 199116 Mean reward for last 10 episodes: 20.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 199616 Mean reward for last 10 episodes: 21.2 Epsilon: 0.09999999999985551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 200116 Mean reward for last 10 episodes: 24.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 200616 Mean reward for last 10 episodes: 24.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 201116 Mean reward for last 10 episodes: 23.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 201616 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 202116 Mean reward for last 10 episodes: 20.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 202616 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 203116 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 203616 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 204116 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 204616 Mean reward for last 10 episodes: 22.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 205116 Mean reward for last 10 episodes: 25.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 205616 Mean reward for last 10 episodes: 21.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 206116 Mean reward for last 10 episodes: 22.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 206616 Mean reward for last 10 episodes: 22.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 207116 Mean reward for last 10 episodes: 20.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 207616 Mean reward for last 10 episodes: 24.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 208116 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 208616 Mean reward for last 10 episodes: 20.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 209116 Mean reward for last 10 episodes: 21.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 209616 Mean reward for last 10 episodes: 22.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 210116 Mean reward for last 10 episodes: 22.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 210616 Mean reward for last 10 episodes: 23.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 211116 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 211616 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 212116 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Saved model at episode number: 4000\n",
      "Total steps: 212616 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 213116 Mean reward for last 10 episodes: 22.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 213616 Mean reward for last 10 episodes: 19.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 214116 Mean reward for last 10 episodes: 23.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 214616 Mean reward for last 10 episodes: 22.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 215116 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 215616 Mean reward for last 10 episodes: 24.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 216116 Mean reward for last 10 episodes: 20.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 216616 Mean reward for last 10 episodes: 22.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 217116 Mean reward for last 10 episodes: 19.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 217616 Mean reward for last 10 episodes: 19.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 218116 Mean reward for last 10 episodes: 19.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 218616 Mean reward for last 10 episodes: 21.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 219116 Mean reward for last 10 episodes: 22.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 219616 Mean reward for last 10 episodes: 21.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 220116 Mean reward for last 10 episodes: 21.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 220616 Mean reward for last 10 episodes: 23.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 221116 Mean reward for last 10 episodes: 24.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 221616 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 222116 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 222616 Mean reward for last 10 episodes: 20.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 223116 Mean reward for last 10 episodes: 23.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 223616 Mean reward for last 10 episodes: 22.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 224116 Mean reward for last 10 episodes: 20.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 224616 Mean reward for last 10 episodes: 21.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 225116 Mean reward for last 10 episodes: 20.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 225616 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 226116 Mean reward for last 10 episodes: 21.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 226616 Mean reward for last 10 episodes: 21.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 227116 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 227616 Mean reward for last 10 episodes: 21.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 228116 Mean reward for last 10 episodes: 23.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 228616 Mean reward for last 10 episodes: 23.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 229116 Mean reward for last 10 episodes: 23.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 229616 Mean reward for last 10 episodes: 23.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 230116 Mean reward for last 10 episodes: 22.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 230616 Mean reward for last 10 episodes: 23.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 231116 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 231616 Mean reward for last 10 episodes: 25.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 232116 Mean reward for last 10 episodes: 21.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 232616 Mean reward for last 10 episodes: 21.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 233116 Mean reward for last 10 episodes: 22.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 233616 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 234116 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 234616 Mean reward for last 10 episodes: 21.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 235116 Mean reward for last 10 episodes: 22.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 235616 Mean reward for last 10 episodes: 23.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 236116 Mean reward for last 10 episodes: 17.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 236616 Mean reward for last 10 episodes: 19.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 237116 Mean reward for last 10 episodes: 22.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 237616 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 238116 Mean reward for last 10 episodes: 21.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 238616 Mean reward for last 10 episodes: 22.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 239116 Mean reward for last 10 episodes: 22.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 239616 Mean reward for last 10 episodes: 20.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 240116 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 240616 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 241116 Mean reward for last 10 episodes: 21.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 241616 Mean reward for last 10 episodes: 23.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 242116 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 242616 Mean reward for last 10 episodes: 24.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 243116 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 243616 Mean reward for last 10 episodes: 24.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 244116 Mean reward for last 10 episodes: 21.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 244616 Mean reward for last 10 episodes: 25.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 245116 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 245616 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 246116 Mean reward for last 10 episodes: 21.1 Epsilon: 0.09999999999985551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 246616 Mean reward for last 10 episodes: 21.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 247116 Mean reward for last 10 episodes: 23.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 247616 Mean reward for last 10 episodes: 21.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 248116 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 248616 Mean reward for last 10 episodes: 21.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 249116 Mean reward for last 10 episodes: 22.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 249616 Mean reward for last 10 episodes: 21.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 250116 Mean reward for last 10 episodes: 21.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 250616 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 251116 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 251616 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 252116 Mean reward for last 10 episodes: 21.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 252616 Mean reward for last 10 episodes: 22.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 253116 Mean reward for last 10 episodes: 22.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 253616 Mean reward for last 10 episodes: 23.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 254116 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 254616 Mean reward for last 10 episodes: 21.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 255116 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 255616 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 256116 Mean reward for last 10 episodes: 19.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 256616 Mean reward for last 10 episodes: 21.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 257116 Mean reward for last 10 episodes: 18.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 257616 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 258116 Mean reward for last 10 episodes: 21.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 258616 Mean reward for last 10 episodes: 22.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 259116 Mean reward for last 10 episodes: 25.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 259616 Mean reward for last 10 episodes: 22.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 260116 Mean reward for last 10 episodes: 22.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 260616 Mean reward for last 10 episodes: 21.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 261116 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 261616 Mean reward for last 10 episodes: 24.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 262116 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Saved model at episode number: 5000\n",
      "Total steps: 262616 Mean reward for last 10 episodes: 24.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 263116 Mean reward for last 10 episodes: 20.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 263616 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 264116 Mean reward for last 10 episodes: 23.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 264616 Mean reward for last 10 episodes: 21.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 265116 Mean reward for last 10 episodes: 21.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 265616 Mean reward for last 10 episodes: 23.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 266116 Mean reward for last 10 episodes: 22.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 266616 Mean reward for last 10 episodes: 23.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 267116 Mean reward for last 10 episodes: 23.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 267616 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 268116 Mean reward for last 10 episodes: 23.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 268616 Mean reward for last 10 episodes: 22.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 269116 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 269616 Mean reward for last 10 episodes: 22.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 270116 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 270616 Mean reward for last 10 episodes: 22.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 271116 Mean reward for last 10 episodes: 22.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 271616 Mean reward for last 10 episodes: 22.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 272116 Mean reward for last 10 episodes: 23.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 272616 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 273116 Mean reward for last 10 episodes: 23.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 273616 Mean reward for last 10 episodes: 22.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 274116 Mean reward for last 10 episodes: 24.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 274616 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 275116 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 275616 Mean reward for last 10 episodes: 23.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 276116 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 276616 Mean reward for last 10 episodes: 23.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 277116 Mean reward for last 10 episodes: 24.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 277616 Mean reward for last 10 episodes: 24.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 278116 Mean reward for last 10 episodes: 21.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 278616 Mean reward for last 10 episodes: 21.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 279116 Mean reward for last 10 episodes: 23.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 279616 Mean reward for last 10 episodes: 25.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 280116 Mean reward for last 10 episodes: 22.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 280616 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 281116 Mean reward for last 10 episodes: 22.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 281616 Mean reward for last 10 episodes: 21.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 282116 Mean reward for last 10 episodes: 23.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 282616 Mean reward for last 10 episodes: 22.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 283116 Mean reward for last 10 episodes: 22.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 283616 Mean reward for last 10 episodes: 23.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 284116 Mean reward for last 10 episodes: 21.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 284616 Mean reward for last 10 episodes: 18.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 285116 Mean reward for last 10 episodes: 21.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 285616 Mean reward for last 10 episodes: 19.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 286116 Mean reward for last 10 episodes: 22.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 286616 Mean reward for last 10 episodes: 23.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 287116 Mean reward for last 10 episodes: 22.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 287616 Mean reward for last 10 episodes: 22.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 288116 Mean reward for last 10 episodes: 21.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 288616 Mean reward for last 10 episodes: 21.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 289116 Mean reward for last 10 episodes: 22.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 289616 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 290116 Mean reward for last 10 episodes: 22.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 290616 Mean reward for last 10 episodes: 23.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 291116 Mean reward for last 10 episodes: 25.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 291616 Mean reward for last 10 episodes: 19.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 292116 Mean reward for last 10 episodes: 22.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 292616 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 293116 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 293616 Mean reward for last 10 episodes: 19.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 294116 Mean reward for last 10 episodes: 21.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 294616 Mean reward for last 10 episodes: 23.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 295116 Mean reward for last 10 episodes: 23.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 295616 Mean reward for last 10 episodes: 21.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 296116 Mean reward for last 10 episodes: 23.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 296616 Mean reward for last 10 episodes: 22.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 297116 Mean reward for last 10 episodes: 21.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 297616 Mean reward for last 10 episodes: 23.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 298116 Mean reward for last 10 episodes: 20.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 298616 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 299116 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 299616 Mean reward for last 10 episodes: 24.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 300116 Mean reward for last 10 episodes: 19.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 300616 Mean reward for last 10 episodes: 21.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 301116 Mean reward for last 10 episodes: 23.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 301616 Mean reward for last 10 episodes: 20.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 302116 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 302616 Mean reward for last 10 episodes: 21.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 303116 Mean reward for last 10 episodes: 24.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 303616 Mean reward for last 10 episodes: 23.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 304116 Mean reward for last 10 episodes: 20.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 304616 Mean reward for last 10 episodes: 23.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 305116 Mean reward for last 10 episodes: 21.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 305616 Mean reward for last 10 episodes: 22.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 306116 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 306616 Mean reward for last 10 episodes: 24.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 307116 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 307616 Mean reward for last 10 episodes: 24.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 308116 Mean reward for last 10 episodes: 20.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 308616 Mean reward for last 10 episodes: 23.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 309116 Mean reward for last 10 episodes: 24.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 309616 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 310116 Mean reward for last 10 episodes: 21.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 310616 Mean reward for last 10 episodes: 23.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 311116 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 311616 Mean reward for last 10 episodes: 20.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 312116 Mean reward for last 10 episodes: 23.6 Epsilon: 0.09999999999985551\n",
      "Saved model at episode number: 6000\n",
      "Total steps: 312616 Mean reward for last 10 episodes: 21.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 313116 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 313616 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 314116 Mean reward for last 10 episodes: 22.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 314616 Mean reward for last 10 episodes: 22.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 315116 Mean reward for last 10 episodes: 24.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 315616 Mean reward for last 10 episodes: 23.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 316116 Mean reward for last 10 episodes: 20.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 316616 Mean reward for last 10 episodes: 22.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 317116 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 317616 Mean reward for last 10 episodes: 23.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 318116 Mean reward for last 10 episodes: 22.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 318616 Mean reward for last 10 episodes: 23.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 319116 Mean reward for last 10 episodes: 24.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 319616 Mean reward for last 10 episodes: 24.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 320116 Mean reward for last 10 episodes: 22.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 320616 Mean reward for last 10 episodes: 24.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 321116 Mean reward for last 10 episodes: 25.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 321616 Mean reward for last 10 episodes: 20.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 322116 Mean reward for last 10 episodes: 24.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 322616 Mean reward for last 10 episodes: 24.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 323116 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 323616 Mean reward for last 10 episodes: 21.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 324116 Mean reward for last 10 episodes: 23.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 324616 Mean reward for last 10 episodes: 20.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 325116 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 325616 Mean reward for last 10 episodes: 19.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 326116 Mean reward for last 10 episodes: 21.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 326616 Mean reward for last 10 episodes: 22.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 327116 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 327616 Mean reward for last 10 episodes: 22.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 328116 Mean reward for last 10 episodes: 20.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 328616 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 329116 Mean reward for last 10 episodes: 22.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 329616 Mean reward for last 10 episodes: 25.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 330116 Mean reward for last 10 episodes: 23.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 330616 Mean reward for last 10 episodes: 21.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 331116 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 331616 Mean reward for last 10 episodes: 21.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 332116 Mean reward for last 10 episodes: 22.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 332616 Mean reward for last 10 episodes: 22.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 333116 Mean reward for last 10 episodes: 21.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 333616 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 334116 Mean reward for last 10 episodes: 21.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 334616 Mean reward for last 10 episodes: 24.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 335116 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 335616 Mean reward for last 10 episodes: 23.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 336116 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 336616 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 337116 Mean reward for last 10 episodes: 19.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 337616 Mean reward for last 10 episodes: 23.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 338116 Mean reward for last 10 episodes: 23.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 338616 Mean reward for last 10 episodes: 21.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 339116 Mean reward for last 10 episodes: 23.4 Epsilon: 0.09999999999985551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 339616 Mean reward for last 10 episodes: 24.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 340116 Mean reward for last 10 episodes: 22.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 340616 Mean reward for last 10 episodes: 22.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 341116 Mean reward for last 10 episodes: 20.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 341616 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 342116 Mean reward for last 10 episodes: 21.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 342616 Mean reward for last 10 episodes: 24.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 343116 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 343616 Mean reward for last 10 episodes: 19.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 344116 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 344616 Mean reward for last 10 episodes: 22.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 345116 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 345616 Mean reward for last 10 episodes: 21.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 346116 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 346616 Mean reward for last 10 episodes: 19.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 347116 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 347616 Mean reward for last 10 episodes: 21.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 348116 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 348616 Mean reward for last 10 episodes: 21.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 349116 Mean reward for last 10 episodes: 22.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 349616 Mean reward for last 10 episodes: 25.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 350116 Mean reward for last 10 episodes: 20.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 350616 Mean reward for last 10 episodes: 21.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 351116 Mean reward for last 10 episodes: 23.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 351616 Mean reward for last 10 episodes: 23.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 352116 Mean reward for last 10 episodes: 22.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 352616 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 353116 Mean reward for last 10 episodes: 23.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 353616 Mean reward for last 10 episodes: 25.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 354116 Mean reward for last 10 episodes: 24.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 354616 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 355116 Mean reward for last 10 episodes: 21.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 355616 Mean reward for last 10 episodes: 21.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 356116 Mean reward for last 10 episodes: 23.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 356616 Mean reward for last 10 episodes: 23.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 357116 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 357616 Mean reward for last 10 episodes: 23.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 358116 Mean reward for last 10 episodes: 19.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 358616 Mean reward for last 10 episodes: 22.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 359116 Mean reward for last 10 episodes: 22.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 359616 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 360116 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 360616 Mean reward for last 10 episodes: 23.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 361116 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 361616 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 362116 Mean reward for last 10 episodes: 22.3 Epsilon: 0.09999999999985551\n",
      "Saved model at episode number: 7000\n",
      "Total steps: 362616 Mean reward for last 10 episodes: 22.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 363116 Mean reward for last 10 episodes: 22.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 363616 Mean reward for last 10 episodes: 23.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 364116 Mean reward for last 10 episodes: 20.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 364616 Mean reward for last 10 episodes: 23.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 365116 Mean reward for last 10 episodes: 24.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 365616 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 366116 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 366616 Mean reward for last 10 episodes: 24.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 367116 Mean reward for last 10 episodes: 20.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 367616 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 368116 Mean reward for last 10 episodes: 21.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 368616 Mean reward for last 10 episodes: 19.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 369116 Mean reward for last 10 episodes: 20.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 369616 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 370116 Mean reward for last 10 episodes: 22.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 370616 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 371116 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 371616 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 372116 Mean reward for last 10 episodes: 20.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 372616 Mean reward for last 10 episodes: 23.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 373116 Mean reward for last 10 episodes: 20.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 373616 Mean reward for last 10 episodes: 22.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 374116 Mean reward for last 10 episodes: 22.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 374616 Mean reward for last 10 episodes: 23.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 375116 Mean reward for last 10 episodes: 21.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 375616 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 376116 Mean reward for last 10 episodes: 22.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 376616 Mean reward for last 10 episodes: 25.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 377116 Mean reward for last 10 episodes: 23.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 377616 Mean reward for last 10 episodes: 21.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 378116 Mean reward for last 10 episodes: 22.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 378616 Mean reward for last 10 episodes: 19.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 379116 Mean reward for last 10 episodes: 21.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 379616 Mean reward for last 10 episodes: 23.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 380116 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 380616 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 381116 Mean reward for last 10 episodes: 23.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 381616 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 382116 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 382616 Mean reward for last 10 episodes: 23.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 383116 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 383616 Mean reward for last 10 episodes: 23.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 384116 Mean reward for last 10 episodes: 20.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 384616 Mean reward for last 10 episodes: 23.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 385116 Mean reward for last 10 episodes: 23.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 385616 Mean reward for last 10 episodes: 22.1 Epsilon: 0.09999999999985551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 386116 Mean reward for last 10 episodes: 23.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 386616 Mean reward for last 10 episodes: 20.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 387116 Mean reward for last 10 episodes: 23.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 387616 Mean reward for last 10 episodes: 24.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 388116 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 388616 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 389116 Mean reward for last 10 episodes: 24.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 389616 Mean reward for last 10 episodes: 22.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 390116 Mean reward for last 10 episodes: 19.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 390616 Mean reward for last 10 episodes: 20.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 391116 Mean reward for last 10 episodes: 22.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 391616 Mean reward for last 10 episodes: 23.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 392116 Mean reward for last 10 episodes: 22.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 392616 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 393116 Mean reward for last 10 episodes: 24.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 393616 Mean reward for last 10 episodes: 23.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 394116 Mean reward for last 10 episodes: 24.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 394616 Mean reward for last 10 episodes: 20.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 395116 Mean reward for last 10 episodes: 24.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 395616 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 396116 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 396616 Mean reward for last 10 episodes: 22.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 397116 Mean reward for last 10 episodes: 22.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 397616 Mean reward for last 10 episodes: 24.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 398116 Mean reward for last 10 episodes: 23.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 398616 Mean reward for last 10 episodes: 21.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 399116 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 399616 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 400116 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 400616 Mean reward for last 10 episodes: 21.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 401116 Mean reward for last 10 episodes: 23.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 401616 Mean reward for last 10 episodes: 24.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 402116 Mean reward for last 10 episodes: 22.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 402616 Mean reward for last 10 episodes: 23.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 403116 Mean reward for last 10 episodes: 23.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 403616 Mean reward for last 10 episodes: 21.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 404116 Mean reward for last 10 episodes: 22.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 404616 Mean reward for last 10 episodes: 25.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 405116 Mean reward for last 10 episodes: 22.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 405616 Mean reward for last 10 episodes: 23.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 406116 Mean reward for last 10 episodes: 23.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 406616 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 407116 Mean reward for last 10 episodes: 23.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 407616 Mean reward for last 10 episodes: 24.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 408116 Mean reward for last 10 episodes: 24.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 408616 Mean reward for last 10 episodes: 23.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 409116 Mean reward for last 10 episodes: 22.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 409616 Mean reward for last 10 episodes: 25.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 410116 Mean reward for last 10 episodes: 21.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 410616 Mean reward for last 10 episodes: 20.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 411116 Mean reward for last 10 episodes: 23.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 411616 Mean reward for last 10 episodes: 23.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 412116 Mean reward for last 10 episodes: 20.6 Epsilon: 0.09999999999985551\n",
      "Saved model at episode number: 8000\n",
      "Total steps: 412616 Mean reward for last 10 episodes: 21.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 413116 Mean reward for last 10 episodes: 23.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 413616 Mean reward for last 10 episodes: 24.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 414116 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 414616 Mean reward for last 10 episodes: 23.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 415116 Mean reward for last 10 episodes: 22.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 415616 Mean reward for last 10 episodes: 24.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 416116 Mean reward for last 10 episodes: 24.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 416616 Mean reward for last 10 episodes: 23.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 417116 Mean reward for last 10 episodes: 20.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 417616 Mean reward for last 10 episodes: 21.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 418116 Mean reward for last 10 episodes: 23.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 418616 Mean reward for last 10 episodes: 22.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 419116 Mean reward for last 10 episodes: 24.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 419616 Mean reward for last 10 episodes: 21.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 420116 Mean reward for last 10 episodes: 23.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 420616 Mean reward for last 10 episodes: 22.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 421116 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 421616 Mean reward for last 10 episodes: 21.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 422116 Mean reward for last 10 episodes: 22.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 422616 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 423116 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 423616 Mean reward for last 10 episodes: 21.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 424116 Mean reward for last 10 episodes: 24.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 424616 Mean reward for last 10 episodes: 23.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 425116 Mean reward for last 10 episodes: 24.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 425616 Mean reward for last 10 episodes: 19.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 426116 Mean reward for last 10 episodes: 21.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 426616 Mean reward for last 10 episodes: 24.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 427116 Mean reward for last 10 episodes: 21.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 427616 Mean reward for last 10 episodes: 20.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 428116 Mean reward for last 10 episodes: 23.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 428616 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 429116 Mean reward for last 10 episodes: 23.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 429616 Mean reward for last 10 episodes: 21.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 430116 Mean reward for last 10 episodes: 24.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 430616 Mean reward for last 10 episodes: 23.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 431116 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 431616 Mean reward for last 10 episodes: 24.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 432116 Mean reward for last 10 episodes: 21.8 Epsilon: 0.09999999999985551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 432616 Mean reward for last 10 episodes: 23.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 433116 Mean reward for last 10 episodes: 21.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 433616 Mean reward for last 10 episodes: 20.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 434116 Mean reward for last 10 episodes: 21.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 434616 Mean reward for last 10 episodes: 23.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 435116 Mean reward for last 10 episodes: 23.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 435616 Mean reward for last 10 episodes: 24.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 436116 Mean reward for last 10 episodes: 23.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 436616 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 437116 Mean reward for last 10 episodes: 22.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 437616 Mean reward for last 10 episodes: 23.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 438116 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 438616 Mean reward for last 10 episodes: 23.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 439116 Mean reward for last 10 episodes: 22.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 439616 Mean reward for last 10 episodes: 22.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 440116 Mean reward for last 10 episodes: 22.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 440616 Mean reward for last 10 episodes: 22.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 441116 Mean reward for last 10 episodes: 20.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 441616 Mean reward for last 10 episodes: 22.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 442116 Mean reward for last 10 episodes: 22.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 442616 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 443116 Mean reward for last 10 episodes: 19.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 443616 Mean reward for last 10 episodes: 23.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 444116 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 444616 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 445116 Mean reward for last 10 episodes: 21.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 445616 Mean reward for last 10 episodes: 19.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 446116 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 446616 Mean reward for last 10 episodes: 21.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 447116 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 447616 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 448116 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 448616 Mean reward for last 10 episodes: 23.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 449116 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 449616 Mean reward for last 10 episodes: 22.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 450116 Mean reward for last 10 episodes: 25.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 450616 Mean reward for last 10 episodes: 22.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 451116 Mean reward for last 10 episodes: 24.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 451616 Mean reward for last 10 episodes: 25.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 452116 Mean reward for last 10 episodes: 22.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 452616 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 453116 Mean reward for last 10 episodes: 24.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 453616 Mean reward for last 10 episodes: 21.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 454116 Mean reward for last 10 episodes: 23.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 454616 Mean reward for last 10 episodes: 23.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 455116 Mean reward for last 10 episodes: 24.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 455616 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 456116 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 456616 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 457116 Mean reward for last 10 episodes: 20.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 457616 Mean reward for last 10 episodes: 24.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 458116 Mean reward for last 10 episodes: 22.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 458616 Mean reward for last 10 episodes: 23.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 459116 Mean reward for last 10 episodes: 22.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 459616 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 460116 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 460616 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 461116 Mean reward for last 10 episodes: 20.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 461616 Mean reward for last 10 episodes: 24.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 462116 Mean reward for last 10 episodes: 22.8 Epsilon: 0.09999999999985551\n",
      "Saved model at episode number: 9000\n",
      "Total steps: 462616 Mean reward for last 10 episodes: 20.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 463116 Mean reward for last 10 episodes: 23.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 463616 Mean reward for last 10 episodes: 23.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 464116 Mean reward for last 10 episodes: 25.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 464616 Mean reward for last 10 episodes: 21.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 465116 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 465616 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 466116 Mean reward for last 10 episodes: 23.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 466616 Mean reward for last 10 episodes: 24.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 467116 Mean reward for last 10 episodes: 24.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 467616 Mean reward for last 10 episodes: 21.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 468116 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 468616 Mean reward for last 10 episodes: 23.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 469116 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 469616 Mean reward for last 10 episodes: 21.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 470116 Mean reward for last 10 episodes: 21.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 470616 Mean reward for last 10 episodes: 23.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 471116 Mean reward for last 10 episodes: 20.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 471616 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 472116 Mean reward for last 10 episodes: 22.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 472616 Mean reward for last 10 episodes: 23.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 473116 Mean reward for last 10 episodes: 23.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 473616 Mean reward for last 10 episodes: 21.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 474116 Mean reward for last 10 episodes: 23.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 474616 Mean reward for last 10 episodes: 23.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 475116 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 475616 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 476116 Mean reward for last 10 episodes: 22.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 476616 Mean reward for last 10 episodes: 21.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 477116 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 477616 Mean reward for last 10 episodes: 21.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 478116 Mean reward for last 10 episodes: 21.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 478616 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 479116 Mean reward for last 10 episodes: 23.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 479616 Mean reward for last 10 episodes: 22.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 480116 Mean reward for last 10 episodes: 20.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 480616 Mean reward for last 10 episodes: 21.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 481116 Mean reward for last 10 episodes: 23.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 481616 Mean reward for last 10 episodes: 21.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 482116 Mean reward for last 10 episodes: 25.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 482616 Mean reward for last 10 episodes: 23.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 483116 Mean reward for last 10 episodes: 23.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 483616 Mean reward for last 10 episodes: 22.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 484116 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 484616 Mean reward for last 10 episodes: 20.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 485116 Mean reward for last 10 episodes: 25.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 485616 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 486116 Mean reward for last 10 episodes: 23.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 486616 Mean reward for last 10 episodes: 20.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 487116 Mean reward for last 10 episodes: 24.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 487616 Mean reward for last 10 episodes: 25.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 488116 Mean reward for last 10 episodes: 22.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 488616 Mean reward for last 10 episodes: 17.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 489116 Mean reward for last 10 episodes: 24.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 489616 Mean reward for last 10 episodes: 20.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 490116 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 490616 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 491116 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 491616 Mean reward for last 10 episodes: 25.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 492116 Mean reward for last 10 episodes: 19.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 492616 Mean reward for last 10 episodes: 21.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 493116 Mean reward for last 10 episodes: 22.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 493616 Mean reward for last 10 episodes: 22.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 494116 Mean reward for last 10 episodes: 24.0 Epsilon: 0.09999999999985551\n",
      "Total steps: 494616 Mean reward for last 10 episodes: 21.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 495116 Mean reward for last 10 episodes: 22.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 495616 Mean reward for last 10 episodes: 24.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 496116 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 496616 Mean reward for last 10 episodes: 24.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 497116 Mean reward for last 10 episodes: 22.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 497616 Mean reward for last 10 episodes: 23.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 498116 Mean reward for last 10 episodes: 23.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 498616 Mean reward for last 10 episodes: 21.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 499116 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 499616 Mean reward for last 10 episodes: 22.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 500116 Mean reward for last 10 episodes: 23.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 500616 Mean reward for last 10 episodes: 24.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 501116 Mean reward for last 10 episodes: 20.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 501616 Mean reward for last 10 episodes: 24.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 502116 Mean reward for last 10 episodes: 24.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 502616 Mean reward for last 10 episodes: 22.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 503116 Mean reward for last 10 episodes: 22.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 503616 Mean reward for last 10 episodes: 22.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 504116 Mean reward for last 10 episodes: 22.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 504616 Mean reward for last 10 episodes: 23.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 505116 Mean reward for last 10 episodes: 18.8 Epsilon: 0.09999999999985551\n",
      "Total steps: 505616 Mean reward for last 10 episodes: 23.9 Epsilon: 0.09999999999985551\n",
      "Total steps: 506116 Mean reward for last 10 episodes: 22.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 506616 Mean reward for last 10 episodes: 19.1 Epsilon: 0.09999999999985551\n",
      "Total steps: 507116 Mean reward for last 10 episodes: 19.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 507616 Mean reward for last 10 episodes: 23.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 508116 Mean reward for last 10 episodes: 23.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 508616 Mean reward for last 10 episodes: 22.5 Epsilon: 0.09999999999985551\n",
      "Total steps: 509116 Mean reward for last 10 episodes: 22.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 509616 Mean reward for last 10 episodes: 23.7 Epsilon: 0.09999999999985551\n",
      "Total steps: 510116 Mean reward for last 10 episodes: 22.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 510616 Mean reward for last 10 episodes: 22.4 Epsilon: 0.09999999999985551\n",
      "Total steps: 511116 Mean reward for last 10 episodes: 24.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 511616 Mean reward for last 10 episodes: 22.2 Epsilon: 0.09999999999985551\n",
      "Total steps: 512116 Mean reward for last 10 episodes: 25.5 Epsilon: 0.09999999999985551\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # For tensor board only\n",
    "    writer = tf.summary.FileWriter(\"output\", sess.graph)\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    # loading model\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    \n",
    "    # training\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        # Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        s = processState(s)  # reshape map to [21168] \n",
    "        d = False\n",
    "        \n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        \n",
    "        # The Q-Network\n",
    "        while j < max_epLength: #If the agent takes longer than 200 moves to reach either of the blocks, end the trial.\n",
    "            j += 1\n",
    "            \n",
    "            # Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0,4)\n",
    "            else:\n",
    "                # feed current state to mainQN.scalarInput and get action by predict \"mainQN.predict\"\n",
    "                a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[0]\n",
    "                \n",
    "            s1,r,d = env.step(a)  # new state, reward, done or not\n",
    "            s1 = processState(s1) # reshape map to [21168]\n",
    "            total_steps += 1\n",
    "            \n",
    "            # Save the experience to our episode buffer\n",
    "            episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5])) \n",
    "            \n",
    "            # In case execute more than pre_train_steps\n",
    "            if total_steps > pre_train_steps:\n",
    "                \n",
    "                # update epsilon\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                \n",
    "                # Only replay before each \"update_freq\" steps\n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    \n",
    "                    # Get a random batch of experiences.\n",
    "                    trainBatch = myBuffer.sample(batch_size) \n",
    "                    #Below we perform the Double-DQN update to the target Q-values\n",
    "                    \n",
    "                    # feed experience batch to both networks\n",
    "                    # predict actions\n",
    "                    Q1 = sess.run(\n",
    "                        mainQN.predict, \n",
    "                        feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])}\n",
    "                    )\n",
    "                    # calculate Q-value Q(s', argmax_a[Q(s', a)]), equation (1)\n",
    "                    Q2 = sess.run(\n",
    "                        targetQN.Qout, \n",
    "                        feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])}\n",
    "                    )\n",
    "                    \n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)  # index 4 of batch ~ end game status \n",
    "                    doubleQ = Q2[range(batch_size),Q1]  \n",
    "                    \n",
    "                    # Q-target = R + gamma * Q-value, equation (1)\n",
    "                    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)  # index 2 of batch ~ reward\n",
    "                    \n",
    "                    # Update the network with our target values.\n",
    "                    _ = sess.run(\n",
    "                        mainQN.updateModel, \n",
    "                        feed_dict={\n",
    "                            mainQN.scalarInput:np.vstack(trainBatch[:,0]),\n",
    "                            mainQN.targetQ:targetQ, \n",
    "                            mainQN.actions:trainBatch[:,1]\n",
    "                        }\n",
    "                    )\n",
    "                    \n",
    "                    # Update the target network toward the primary network.\n",
    "                    updateTarget(targetOps,sess) \n",
    "            \n",
    "            # cummulate reward\n",
    "            rAll += r\n",
    "            \n",
    "            # update state\n",
    "            s = s1\n",
    "            \n",
    "            # if game over then break while loop\n",
    "            if d == True:\n",
    "\n",
    "                break\n",
    "        \n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        \n",
    "        jList.append(j)     # list of number steps in each episode\n",
    "        \n",
    "        rList.append(rAll)  # list of reward in each episode\n",
    "        \n",
    "        # eriodically save the model. \n",
    "        if i % 1000 == 0:\n",
    "            saver.save(sess, path+'/model-'+str(i)+'.ckpt')\n",
    "            print(\"Saved model at episode number: %d\" %i)\n",
    "            \n",
    "        \n",
    "        if len(rList) % 10 == 0:\n",
    "            print(\n",
    "                \"Total steps:\", total_steps, \n",
    "                \"Mean reward for last 10 episodes:\", np.mean(rList[-10:]), \n",
    "                \"Epsilon:\", e\n",
    "            )\n",
    "    \n",
    "    # save model at last episode    \n",
    "    saver.save(sess,path+'/model-'+str(i)+'.ckpt')\n",
    "    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking network learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of succesful episodes: 21.4495%\n"
     ]
    }
   ],
   "source": [
    "print(\"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean reward over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa41cf3aac8>]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa41c230b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rMat = np.resize(np.array(rList),[len(rList)//100,100])\n",
    "rMean = np.average(rMat,1)\n",
    "plt.plot(rMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10244"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa41cf8ed68>]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa41c239cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(jList)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
