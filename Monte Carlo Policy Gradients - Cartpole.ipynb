{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole: REINFORCE Monte Carlo Policy Gradients\n",
    "![](https://camo.githubusercontent.com/6c525864040e12c833041d8fffde019ab3c3546e/687474703a2f2f6e6575726f2d6564756361746f722e636f6d2f77702d636f6e74656e742f75706c6f6164732f323031372f30392f44514e2e676966)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import librarires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hung/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env = env.unwrapped\n",
    "\n",
    "# Policy gradient has high variance, seed for reproducability\n",
    "env.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# move left, right\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
      "[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "# Observation space\n",
    "print(env.observation_space.low)\n",
    "print(env.observation_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02725216 -0.04481721 -0.04304738  0.00151751]\n"
     ]
    }
   ],
   "source": [
    "# Maybe, state is define by coordinates of \n",
    "# pole bottom and top points\n",
    "print(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setup hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENV hyper parameters\n",
    "state_size = 4\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Training hyper-parameters\n",
    "max_episodes = 10000\n",
    "learning_rate = 0.01\n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define preprocessing funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(episode_rewards):\n",
    "    \"\"\"\n",
    "    Take the rewards and perform discount\n",
    "    \"\"\"\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    \n",
    "    # G_t = \\sum_{k=0}^{T}\\gamma^k R_{t+k+1}\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "        \n",
    "    # transform to Z-score\n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    discounted_episode_rewards = \\\n",
    "    (discounted_episode_rewards - mean)/(std)\n",
    "    \n",
    "    return discounted_episode_rewards  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Policy Gradient Neural Network model\n",
    "![](https://camo.githubusercontent.com/302679523d9151a5a9ee8093a480096f8cf28f33/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f73696d6f6e696e6974686f6d61732f446565705f7265696e666f7263656d656e745f6c6561726e696e675f436f757273652f6d61737465722f506f6c6963792532304772616469656e74732f43617274706f6c652f6173736574732f636174706f6c652e706e67)\n",
    "\n",
    "- state which is an array of 4 values will be used as an input\n",
    "- NN has 3 FC layers (10-2-2), the last 2 = number of actions\n",
    "- Last layer's activation = softmax = action probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('inputs'):\n",
    "    # input\n",
    "    input_ = tf.placeholder(\n",
    "        dtype=tf.float32, \n",
    "        shape=[None, state_size], \n",
    "        name=\"input_\"\n",
    "    )\n",
    "    # output\n",
    "    actions = tf.placeholder(\n",
    "        dtype=tf.float32, \n",
    "        shape=[None, action_size], \n",
    "        name=\"actions\"\n",
    "    )\n",
    "    discounted_episode_rewards_ = tf.placeholder(\n",
    "        dtype=tf.float32,\n",
    "        shape=[None, ],\n",
    "        name=\"discounted_episode_rewards\"\n",
    "    )\n",
    "    \n",
    "    # Add this placeholder for having this variable in tensorboard\n",
    "    mean_reward_ = tf.placeholder(\n",
    "        dtype=tf.float32,\n",
    "        name=\"mean_reward_\"\n",
    "    )\n",
    "    \n",
    "    # first FC layer\n",
    "    with tf.name_scope(\"fc1\"):\n",
    "        fc1 = tf.contrib.layers.fully_connected(\n",
    "            inputs=input_,\n",
    "            num_outputs = 10,\n",
    "            activation_fn=tf.nn.relu,\n",
    "            weights_initializer=tf.contrib.layers.xavier_initializer()\n",
    "        )\n",
    "    \n",
    "    # second FC layer\n",
    "    with tf.name_scope(\"fc2\"):\n",
    "        fc2 = tf.contrib.layers.fully_connected(\n",
    "            inputs=fc1,\n",
    "            num_outputs = action_size,\n",
    "            activation_fn=tf.nn.relu,\n",
    "            weights_initializer=tf.contrib.layers.xavier_initializer()\n",
    "        )\n",
    "    \n",
    "    # third FC layer\n",
    "    with tf.name_scope(\"fc3\"):\n",
    "        fc3 = tf.contrib.layers.fully_connected(\n",
    "            inputs=fc2,\n",
    "            num_outputs = action_size,\n",
    "            activation_fn=tf.nn.relu,\n",
    "            weights_initializer=tf.contrib.layers.xavier_initializer()\n",
    "        )\n",
    "\n",
    "    # softamx activator\n",
    "    with tf.name_scope(\"softmax\"):\n",
    "        action_distribution = tf.nn.softmax(logits=fc3)\n",
    "    \n",
    "    # loss function\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # tf.nn.softmax_cross_entropy_with_logits computes \n",
    "        # the cross entropy of the result \n",
    "        # after applying the softmax function\n",
    "        # If you have single-class labels, \n",
    "        # where an object can only belong to one class, \n",
    "        # you might now consider using \n",
    "        # tf.nn.sparse_softmax_cross_entropy_with_logits \n",
    "        # so that you don't have to convert your labels \n",
    "        # to a dense one-hot array.\n",
    "        neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            logits = fc3, \n",
    "            labels = actions\n",
    "        )\n",
    "        loss = tf.reduce_mean(neg_log_prob * \n",
    "                              discounted_episode_rewards_)\n",
    "    \n",
    "    # training operator\n",
    "    with tf.name_scope(\"train\"):\n",
    "        train_opt = tf.train.AdamOptimizer(learning_rate)\\\n",
    "                      .minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'inputs_1/softmax/Softmax:0' shape=(?, 2) dtype=float32>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Set up Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"output1\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", loss)\n",
    "\n",
    "## Reward mean\n",
    "tf.summary.scalar(\"Reward_mean\", mean_reward_)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 \tSteps: 18 \tReward: 18.00 \tMean Reward: 18.00 \tMax reward so far: 18.00\n",
      "Episode: 100 \tSteps: 15 \tReward: 15.00 \tMean Reward: 22.00 \tMax reward so far: 84.00\n",
      "Episode: 200 \tSteps: 47 \tReward: 47.00 \tMean Reward: 23.13 \tMax reward so far: 84.00\n",
      "Episode: 300 \tSteps: 17 \tReward: 17.00 \tMean Reward: 23.43 \tMax reward so far: 84.00\n",
      "Episode: 400 \tSteps: 10 \tReward: 10.00 \tMean Reward: 23.68 \tMax reward so far: 84.00\n",
      "Episode: 500 \tSteps: 20 \tReward: 20.00 \tMean Reward: 23.23 \tMax reward so far: 84.00\n",
      "Episode: 600 \tSteps: 15 \tReward: 15.00 \tMean Reward: 23.28 \tMax reward so far: 84.00\n",
      "Episode: 700 \tSteps: 29 \tReward: 29.00 \tMean Reward: 23.01 \tMax reward so far: 90.00\n",
      "Episode: 800 \tSteps: 17 \tReward: 17.00 \tMean Reward: 23.02 \tMax reward so far: 90.00\n",
      "Episode: 900 \tSteps: 37 \tReward: 37.00 \tMean Reward: 22.99 \tMax reward so far: 90.00\n",
      "Episode: 1000 \tSteps: 12 \tReward: 12.00 \tMean Reward: 22.89 \tMax reward so far: 90.00\n",
      "Episode: 1100 \tSteps: 17 \tReward: 17.00 \tMean Reward: 22.86 \tMax reward so far: 90.00\n",
      "Episode: 1200 \tSteps: 12 \tReward: 12.00 \tMean Reward: 22.80 \tMax reward so far: 90.00\n",
      "Episode: 1300 \tSteps: 12 \tReward: 12.00 \tMean Reward: 22.82 \tMax reward so far: 90.00\n",
      "Episode: 1400 \tSteps: 23 \tReward: 23.00 \tMean Reward: 22.73 \tMax reward so far: 90.00\n",
      "Episode: 1500 \tSteps: 13 \tReward: 13.00 \tMean Reward: 22.55 \tMax reward so far: 90.00\n",
      "Episode: 1600 \tSteps: 14 \tReward: 14.00 \tMean Reward: 22.55 \tMax reward so far: 90.00\n",
      "Episode: 1700 \tSteps: 30 \tReward: 30.00 \tMean Reward: 22.56 \tMax reward so far: 90.00\n",
      "Episode: 1800 \tSteps: 18 \tReward: 18.00 \tMean Reward: 22.51 \tMax reward so far: 90.00\n",
      "Episode: 1900 \tSteps: 44 \tReward: 44.00 \tMean Reward: 22.51 \tMax reward so far: 90.00\n",
      "Episode: 2000 \tSteps: 29 \tReward: 29.00 \tMean Reward: 22.53 \tMax reward so far: 90.00\n",
      "Episode: 2100 \tSteps: 18 \tReward: 18.00 \tMean Reward: 22.49 \tMax reward so far: 90.00\n",
      "Episode: 2200 \tSteps: 15 \tReward: 15.00 \tMean Reward: 22.50 \tMax reward so far: 117.00\n",
      "Episode: 2300 \tSteps: 57 \tReward: 57.00 \tMean Reward: 22.56 \tMax reward so far: 117.00\n",
      "Episode: 2400 \tSteps: 13 \tReward: 13.00 \tMean Reward: 22.62 \tMax reward so far: 117.00\n",
      "Episode: 2500 \tSteps: 22 \tReward: 22.00 \tMean Reward: 22.59 \tMax reward so far: 117.00\n",
      "Episode: 2600 \tSteps: 14 \tReward: 14.00 \tMean Reward: 22.60 \tMax reward so far: 117.00\n",
      "Episode: 2700 \tSteps: 18 \tReward: 18.00 \tMean Reward: 22.60 \tMax reward so far: 117.00\n",
      "Episode: 2800 \tSteps: 13 \tReward: 13.00 \tMean Reward: 22.59 \tMax reward so far: 117.00\n",
      "Episode: 2900 \tSteps: 13 \tReward: 13.00 \tMean Reward: 22.58 \tMax reward so far: 117.00\n",
      "Episode: 3000 \tSteps: 19 \tReward: 19.00 \tMean Reward: 22.48 \tMax reward so far: 117.00\n",
      "Episode: 3100 \tSteps: 12 \tReward: 12.00 \tMean Reward: 22.56 \tMax reward so far: 117.00\n",
      "Episode: 3200 \tSteps: 14 \tReward: 14.00 \tMean Reward: 22.51 \tMax reward so far: 117.00\n",
      "Episode: 3300 \tSteps: 17 \tReward: 17.00 \tMean Reward: 22.52 \tMax reward so far: 117.00\n",
      "Episode: 3400 \tSteps: 13 \tReward: 13.00 \tMean Reward: 22.51 \tMax reward so far: 117.00\n",
      "Episode: 3500 \tSteps: 22 \tReward: 22.00 \tMean Reward: 22.53 \tMax reward so far: 117.00\n",
      "Episode: 3600 \tSteps: 11 \tReward: 11.00 \tMean Reward: 22.56 \tMax reward so far: 117.00\n",
      "Episode: 3700 \tSteps: 20 \tReward: 20.00 \tMean Reward: 22.54 \tMax reward so far: 117.00\n",
      "Episode: 3800 \tSteps: 19 \tReward: 19.00 \tMean Reward: 22.57 \tMax reward so far: 117.00\n",
      "Episode: 3900 \tSteps: 31 \tReward: 31.00 \tMean Reward: 22.54 \tMax reward so far: 117.00\n",
      "Episode: 4000 \tSteps: 18 \tReward: 18.00 \tMean Reward: 22.61 \tMax reward so far: 117.00\n",
      "Episode: 4100 \tSteps: 33 \tReward: 33.00 \tMean Reward: 22.59 \tMax reward so far: 117.00\n",
      "Episode: 4200 \tSteps: 36 \tReward: 36.00 \tMean Reward: 22.58 \tMax reward so far: 117.00\n",
      "Episode: 4300 \tSteps: 35 \tReward: 35.00 \tMean Reward: 22.54 \tMax reward so far: 117.00\n",
      "Episode: 4400 \tSteps: 19 \tReward: 19.00 \tMean Reward: 22.49 \tMax reward so far: 117.00\n",
      "Episode: 4500 \tSteps: 21 \tReward: 21.00 \tMean Reward: 22.49 \tMax reward so far: 117.00\n",
      "Episode: 4600 \tSteps: 55 \tReward: 55.00 \tMean Reward: 22.51 \tMax reward so far: 117.00\n",
      "Episode: 4700 \tSteps: 12 \tReward: 12.00 \tMean Reward: 22.47 \tMax reward so far: 117.00\n",
      "Episode: 4800 \tSteps: 38 \tReward: 38.00 \tMean Reward: 22.48 \tMax reward so far: 117.00\n",
      "Episode: 4900 \tSteps: 9 \tReward: 9.00 \tMean Reward: 22.48 \tMax reward so far: 117.00\n",
      "Episode: 5000 \tSteps: 22 \tReward: 22.00 \tMean Reward: 22.49 \tMax reward so far: 117.00\n",
      "Episode: 5100 \tSteps: 23 \tReward: 23.00 \tMean Reward: 22.49 \tMax reward so far: 117.00\n",
      "Episode: 5200 \tSteps: 22 \tReward: 22.00 \tMean Reward: 22.53 \tMax reward so far: 117.00\n",
      "Episode: 5300 \tSteps: 23 \tReward: 23.00 \tMean Reward: 22.55 \tMax reward so far: 117.00\n",
      "Episode: 5400 \tSteps: 74 \tReward: 74.00 \tMean Reward: 22.55 \tMax reward so far: 117.00\n",
      "Episode: 5500 \tSteps: 11 \tReward: 11.00 \tMean Reward: 22.52 \tMax reward so far: 117.00\n",
      "Episode: 5600 \tSteps: 12 \tReward: 12.00 \tMean Reward: 22.50 \tMax reward so far: 117.00\n",
      "Episode: 5700 \tSteps: 14 \tReward: 14.00 \tMean Reward: 22.51 \tMax reward so far: 117.00\n",
      "Episode: 5800 \tSteps: 43 \tReward: 43.00 \tMean Reward: 22.54 \tMax reward so far: 117.00\n",
      "Episode: 5900 \tSteps: 16 \tReward: 16.00 \tMean Reward: 22.50 \tMax reward so far: 117.00\n",
      "Episode: 6000 \tSteps: 23 \tReward: 23.00 \tMean Reward: 22.47 \tMax reward so far: 117.00\n",
      "Episode: 6100 \tSteps: 63 \tReward: 63.00 \tMean Reward: 22.44 \tMax reward so far: 117.00\n",
      "Episode: 6200 \tSteps: 13 \tReward: 13.00 \tMean Reward: 22.43 \tMax reward so far: 117.00\n",
      "Episode: 6300 \tSteps: 20 \tReward: 20.00 \tMean Reward: 22.42 \tMax reward so far: 117.00\n",
      "Episode: 6400 \tSteps: 44 \tReward: 44.00 \tMean Reward: 22.45 \tMax reward so far: 117.00\n",
      "Episode: 6500 \tSteps: 11 \tReward: 11.00 \tMean Reward: 22.44 \tMax reward so far: 117.00\n",
      "Episode: 6600 \tSteps: 14 \tReward: 14.00 \tMean Reward: 22.43 \tMax reward so far: 117.00\n",
      "Episode: 6700 \tSteps: 12 \tReward: 12.00 \tMean Reward: 22.42 \tMax reward so far: 117.00\n",
      "Episode: 6800 \tSteps: 15 \tReward: 15.00 \tMean Reward: 22.40 \tMax reward so far: 117.00\n",
      "Episode: 6900 \tSteps: 12 \tReward: 12.00 \tMean Reward: 22.38 \tMax reward so far: 117.00\n",
      "Episode: 7000 \tSteps: 10 \tReward: 10.00 \tMean Reward: 22.40 \tMax reward so far: 117.00\n",
      "Episode: 7100 \tSteps: 13 \tReward: 13.00 \tMean Reward: 22.38 \tMax reward so far: 117.00\n",
      "Episode: 7200 \tSteps: 12 \tReward: 12.00 \tMean Reward: 22.38 \tMax reward so far: 117.00\n",
      "Episode: 7300 \tSteps: 13 \tReward: 13.00 \tMean Reward: 22.36 \tMax reward so far: 117.00\n",
      "Episode: 7400 \tSteps: 15 \tReward: 15.00 \tMean Reward: 22.38 \tMax reward so far: 117.00\n",
      "Episode: 7500 \tSteps: 14 \tReward: 14.00 \tMean Reward: 22.39 \tMax reward so far: 117.00\n",
      "Episode: 7600 \tSteps: 26 \tReward: 26.00 \tMean Reward: 22.39 \tMax reward so far: 117.00\n",
      "Episode: 7700 \tSteps: 10 \tReward: 10.00 \tMean Reward: 22.36 \tMax reward so far: 117.00\n",
      "Episode: 7800 \tSteps: 17 \tReward: 17.00 \tMean Reward: 22.34 \tMax reward so far: 117.00\n",
      "Episode: 7900 \tSteps: 14 \tReward: 14.00 \tMean Reward: 22.34 \tMax reward so far: 117.00\n",
      "Episode: 8000 \tSteps: 12 \tReward: 12.00 \tMean Reward: 22.33 \tMax reward so far: 117.00\n",
      "Episode: 8100 \tSteps: 76 \tReward: 76.00 \tMean Reward: 22.34 \tMax reward so far: 117.00\n",
      "Episode: 8200 \tSteps: 46 \tReward: 46.00 \tMean Reward: 22.33 \tMax reward so far: 117.00\n",
      "Episode: 8300 \tSteps: 33 \tReward: 33.00 \tMean Reward: 22.34 \tMax reward so far: 117.00\n",
      "Episode: 8400 \tSteps: 18 \tReward: 18.00 \tMean Reward: 22.33 \tMax reward so far: 117.00\n",
      "Episode: 8500 \tSteps: 15 \tReward: 15.00 \tMean Reward: 22.32 \tMax reward so far: 117.00\n",
      "Episode: 8600 \tSteps: 16 \tReward: 16.00 \tMean Reward: 22.30 \tMax reward so far: 117.00\n",
      "Episode: 8700 \tSteps: 13 \tReward: 13.00 \tMean Reward: 22.29 \tMax reward so far: 117.00\n",
      "Episode: 8800 \tSteps: 17 \tReward: 17.00 \tMean Reward: 22.28 \tMax reward so far: 117.00\n",
      "Episode: 8900 \tSteps: 16 \tReward: 16.00 \tMean Reward: 22.26 \tMax reward so far: 117.00\n",
      "Episode: 9000 \tSteps: 17 \tReward: 17.00 \tMean Reward: 22.28 \tMax reward so far: 117.00\n",
      "Episode: 9100 \tSteps: 59 \tReward: 59.00 \tMean Reward: 22.28 \tMax reward so far: 117.00\n",
      "Episode: 9200 \tSteps: 22 \tReward: 22.00 \tMean Reward: 22.28 \tMax reward so far: 117.00\n",
      "Episode: 9300 \tSteps: 15 \tReward: 15.00 \tMean Reward: 22.28 \tMax reward so far: 117.00\n",
      "Episode: 9400 \tSteps: 29 \tReward: 29.00 \tMean Reward: 22.28 \tMax reward so far: 117.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 9500 \tSteps: 28 \tReward: 28.00 \tMean Reward: 22.28 \tMax reward so far: 117.00\n",
      "Episode: 9600 \tSteps: 18 \tReward: 18.00 \tMean Reward: 22.27 \tMax reward so far: 117.00\n",
      "Episode: 9700 \tSteps: 14 \tReward: 14.00 \tMean Reward: 22.28 \tMax reward so far: 117.00\n",
      "Episode: 9800 \tSteps: 16 \tReward: 16.00 \tMean Reward: 22.27 \tMax reward so far: 117.00\n",
      "Episode: 9900 \tSteps: 34 \tReward: 34.00 \tMean Reward: 22.27 \tMax reward so far: 117.00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "allRewards = []\n",
    "total_rewards = 0\n",
    "maximumRewardRecorded = 0\n",
    "episode = 0\n",
    "episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for episode in range(max_episodes):\n",
    "        \n",
    "        episode_rewards_sum = 0\n",
    "        \n",
    "        # reset the game\n",
    "        state = env.reset()\n",
    "        \n",
    "        # env.render\n",
    "        total_steps = 0\n",
    "        while True:\n",
    "            # tracking step number\n",
    "            total_steps += 1\n",
    "            # Choose an action\n",
    "            # Remember that WE'RE NOT IN A DETERMINISTIC ENV\n",
    "            # WE'RE IN STOCHASTIC ENV, WE OUTPUT PROBABILITIES\n",
    "            \n",
    "            action_probability_distribution = \\\n",
    "            sess.run(\n",
    "                action_distribution,\n",
    "                feed_dict={input_: state.reshape([1,4])}\n",
    "            )\n",
    "            \n",
    "            # Select an action w.r.t the actions probability\n",
    "            action = np.random.choice(\n",
    "                a=range(action_probability_distribution.shape[1]),\n",
    "                p=action_probability_distribution.ravel()\n",
    "            )\n",
    "            \n",
    "            # Perform the action\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Store s, a\n",
    "            episode_states.append(state)\n",
    "            \n",
    "            # For actions because we output only one (the index) \n",
    "            # we need 2 (1 is for the action taken)\n",
    "            # We need [0., 1.] (if we take right) \n",
    "            # not just the index\n",
    "            action_ = np.zeros(action_size)\n",
    "            action_[action] = 1\n",
    "            episode_actions.append(action_)\n",
    "            \n",
    "            episode_rewards.append(reward)\n",
    "            \n",
    "            if done:\n",
    "                # Calculate sum reward\n",
    "                episode_rewards_sum = np.sum(episode_rewards)\n",
    "                allRewards.append(episode_rewards_sum)\n",
    "                total_rewards = np.sum(allRewards)\n",
    "                \n",
    "                # Mean reward\n",
    "                mean_reward = np.divide(total_rewards, episode+1)\n",
    "                maximumRewardRecorded = np.amax(allRewards)\n",
    "                \n",
    "                if episode%100 == 0:\n",
    "                    print(\"Episode: %d\" % episode,\n",
    "                          \"\\tSteps: %d\" % total_steps,\n",
    "                          \"\\tReward: %0.2f\" % episode_rewards_sum,\n",
    "                          \"\\tMean Reward: %0.2f\" % mean_reward,\n",
    "                          \"\\tMax reward so far: %0.2f\" % maximumRewardRecorded\n",
    "                         )\n",
    "                \n",
    "                # Calculate discounted reward\n",
    "                discounted_episode_rewards = \\\n",
    "                discount_and_normalize_rewards(episode_rewards)\n",
    "                \n",
    "                # Feed forward, gradient and backpropagation\n",
    "                loss_, _ = sess.run(\n",
    "                    [loss, train_opt],\n",
    "                    feed_dict={\n",
    "                        input_: np.vstack(np.array(episode_states)),\n",
    "                        actions: np.vstack(np.array(episode_actions)),\n",
    "                        discounted_episode_rewards_: discounted_episode_rewards\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Write TF summaries\n",
    "                summary = sess.run(\n",
    "                    write_op,\n",
    "                    feed_dict={\n",
    "                        input_: np.vstack(np.array(episode_states)),\n",
    "                        actions: np.vstack(np.array(episode_actions)),\n",
    "                        discounted_episode_rewards_: discounted_episode_rewards,\n",
    "                        mean_reward_: mean_reward\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "                \n",
    "                # Reset the transition stores\n",
    "                episode_states, episode_actions, episode_rewards = \\\n",
    "                [],[],[]\n",
    "                \n",
    "                break\n",
    "            \n",
    "            # Assign new state\n",
    "            state = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
