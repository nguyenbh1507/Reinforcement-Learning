{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications of Reiforcement Learning in Real World\n",
    "[Link]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There is no reasoning, no precess if inference or comparison; there is no thinking about things, no putting two and two together; there are no ideas - the animal does not think of the box or of the food or of the act he is to perform.* -- **Edward Thorndike** (1874-1949), the psychologits who proposed Law of effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Convolutional Neural Networks (CNN) and Recurrent Neural networks (RNN) are becoming more important for businesses due to their applications in Computer Vision (CV) nad Natural Language Processing (NLP), *Reinforcement Learning* (RL) as a framework for computational neuroscience to model decision making process seems to be undervalued. Besides, there seems to be very little resources detailinf how RL is applied in different industries. Despite the criticsms about RL's weeknesses, RL should never be neglected in the space of corporate research given its huge potentials in assisting decision makin. As Koray Kavukcuoglu, the director of research at DeepMind, said at a conference:\n",
    "\n",
    "\"*If one of the goals that we work for here is AI then it is at the core of that. Reinforcement Learning is a very general framework for learning sequential decision making tasks. And Deep Learning, on the other hand, is of course the best set of algorithms we have to learn representations. And combinations of these two different models is the best answer so far we have in terms of learning very food state representations of very challenging tasks that are not just for solving toy domains but actually to solve challenging real world problems*\"\n",
    "\n",
    "Threrfore this article aims to *1) investigate the breadth and depth of RL applications in real world; 2) view RL from different aspects; and 3) persuade the decision makers and researchers to put more efforts on RL research.*\n",
    "\n",
    "The rest of the article is organized as follows: Section I is general introduction, Section II presents the applications of RL in different domains and brief description of how it was applied, Section III summarizes the things one would need to apply RL, Section IV is the intuition from other disciplines and Section V is about how RL could be useful in the future, Section VI is conclusion. \n",
    "\n",
    "**(* * *)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. introduction to Reinforcement Learning\n",
    "RL, known as a semi-supervised learning model in machine learning, is a technique to allow an agent to take actions and interact with an environment so as to maximize the total rewards. RL is usally modeled as a [Markov Decison Process (MDP)](https://en.wikipedia.org/wiki/Markov_decision_process).\n",
    "![](https://cdn-images-1.medium.com/max/1200/1*-0G8EIeG24OYTbt5KZSalQ.png)\n",
    "Figure 1.1: The agent -environment interaction in a Markov decision making [Source: Reinforcement Learning: An introdcution](http://incompleteideas.net/book/bookdraft2017nov5.pdf))\n",
    "\n",
    "Imagine a bab is given a TV remote control at your home (environment). In simple terms, the baby (agent) will first observe and construct his/her own representation of the environment (state). Then the curious baby will take certain actions like hitting the remote control (actions) and observe how would the TV response (next state). As a non-responding TV is dull, the baby dislike it (receiving a negative reward) and will take less actions that will lead to such a result (updating the policy) and vice versa. The baby will repeat the process until he/she finds a policy (what to do under different circumstances) that he/she is happy with (maximizing the total (discounted) rewards).\n",
    "\n",
    "The study of RL is to construct s mathematical framework to solve the problems. For example, to fnd a good policy use valued-based methods like Q-learning to measure how good an action is in a particular state or policy-based methods to directly find out waht actions to take under different states without knowing how good the actions are.\n",
    "\n",
    "However, the problems we face in the real world can be extremly complicated in many different ways therefore a typical RL algorithm has no clue to solve. For example, the sate space is very large in the game of GO, environment cannot be fully observed in Poker game and there are lots of agents interact with each other in the real world. Researchers have invented methods to solve some of the problems by using deep neural network to model the desired policies, value functions or even the transition models, which therefore is called Deep Reinforcement Learning. This article makes no distinction between RL and Deep RL.\n",
    "\n",
    "There are lots of good stuff about RL online and interested readers can visit [awesome-rl](https://github.com/aikorea/awesome-rl), [argmin](http://www.argmin.net/2018/06/25/outsider-rl/) and [dennybritz](https://github.com/dennybritz/reinforcement-learning).\n",
    "\n",
    "**( * * * )**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. APPLICATIONS\n",
    "This part is written for general readers. At the same time, it will be of greater value for readers with some knowledge about RL.\n",
    "### 2.1. Resources management in computer clusters\n",
    "\n",
    "Designing algorithm to allocate limited resources to different tasks is challenging and requires human-generated heuristics. The paper \"Resource Management with Deep Reinforcement Learning\" [2] showed how to use RL to automatically learn to allocate and schedule computer resources to waiting jobs, with the objective to minimize the average job slowdown.\n",
    "\n",
    "State space was formulated as the current resources allocation and the resources profiles for a job. For action space, they used a trick to allow the agent to choose more than one action at each time step. Reward was the sum of (-1/duration of the job) over all the jobs in the system. Then they combined REINFORCE algorithm and baseline value to calculate the policy gradients and find the best policy parameters that give the probability distribution of actions to minimize the objective. Click here to view the code on [Github](https://github.com/hongzimao/deeprm).\n",
    "\n",
    "### 2.2 Traffic Light Control\n",
    "In the paper \"Reinforcement learning-based multi-agent system for network traffics signal contrl\" [3], researchers tried to design a traffic light controller to solve the congestion problem. Tested only on simulated environment though, their methods showed superior results than tradditional methods and shed light on th potential uses of multi-agent RL in designing traffic system.\n",
    "![](https://cdn-images-1.medium.com/max/1200/1*_ey-4kszbzChGUZMY8Qyuw.png)\n",
    "\n",
    "Five agents were put in the five-intersection traffic network, with a RL agent at the central intersection to control traffic signaling. The state was defined as eight-dimensional vector with each element representing the relative traffic flow of each time. Eight choices were available to the agent, each representing a phase combination, and the reward function was defined as reduction in delay compared with previous time step. The authors used DQN to learn the Q value of the {state, action} pairs.\n",
    "\n",
    "### 2.3 Robotics\n",
    "\n",
    "There are tremendous works on applying RL in robotics. Readers are referred to [10] for asurvey of RL in Robotics. In particular, [11] trained a robot to learn policies to map raw video images to robot's actions. The RGB images were fed to a CNN and outputs were the motor torques. The RL component was the guided policy search to generate training date that came from its own state distribution. \n",
    "\n",
    "A video is available [here](https://www.youtube.com/watch?v=Q4bMcUk6pcw).\n",
    "\n",
    "### 2.4 Web System Configuration\n",
    "\n",
    "There are more than 100 configurable parameters in a web system and the process of tuning the parameters requires a skilled operator and numerous trail-and-error test. The paper \"A Reinforcement Learning Approach to Online Web System Auto-configuration\" [5] showed the first attempt in the domain on how to do autonomic reconfiguration of parameters in multi-tier web system in VM-based dynamic environment.\n",
    "\n",
    "The configuration process can be formulated as a finite MDP. The state space was the system configuration, action sapce {increase, decrease, keep} for each parameter, and reward was defined as the difference between the given targeted response time and measured response time. The authors used the model-free Q-learning algorithm to do the task.\n",
    "\n",
    "Although the authors used some other techniques like policy initialization to remedy the large state space and computational complexity of the problem instead of the potential combinations of RL and neural network, it is belived that the pioneering work has paved the way for future research in this aera.\n",
    "\n",
    "### 2.5 Chemistry\n",
    "![](https://pubs.acs.org/appl/literatum/publisher/achs/journals/content/acscii/2017/acscii.2017.3.issue-12/acscentsci.7b00492/20171220/images/medium/oc-2017-004924_0009.gif)\n",
    "\n",
    "RL can also be applied in optimizing chemical reactions. [4] showed that their model outperformanced state-of-the-art algorithm and generalized to dissimila underlying mechanisms in the paper \"Optimizing Chemical Reactions with Deep Reinforcement Learning\".\n",
    "\n",
    "Combined with LSTM to model the policy function, the RL agent optimized the chemical reaction with the MDP characterized by {S, P, A, R}, where S was the set of experimental conditions (like temperature, pH, etc.), A was the set all possible actions that can change the experimental conditions, P was the transition probability from current experiment condition to the next contition and R was the reward which is a function of the state.\n",
    "\n",
    "[The application](https://github.com/lightingghost/chemopt) is a great one to demonstrate how RL can reduce time-consuming and trial-and-error work in a relatively stable environment.\n",
    "\n",
    "### 2.6 Personalized Recommendation\n",
    "\n",
    "Previous work of news recommendations faced several challenges including the rapid chaging dynamic of news, users get bored easily and Clisk Through Rate cannot reflect the retention rate of users. Guanjie et al. have applied RL in news recommendation system in a paper titled \"DRN: A Deep Reinforcement learning Framework for News Recommendation\" to combat the problems [1].\n",
    "\n",
    "In practice, they constructed four categories features, namely A) user features, and B) context features as the state features of the environment, and C) user-news features and D) news features as the action features. The four features were input to the Deep Q-Network (DQN) to calculate the Q-value. A list of news were chosen to recommend based on the Q-valued, and the user's click on the news was a part of the reward ten RL agent received.\n",
    "\n",
    "The authors also employ other techniques to address other challenging problems, including memory replay, suvival models, Dueling Bandit Gradient Descent and so on.\n",
    "\n",
    "### 2.7 Bidding and Advertising\n",
    "Researchers from Alibaba Group published a paper \"Real-time Bidding with Multi-Agent Reinforcement Learning in Display Advertising\"[6] and cliamed that their distributed cluster-based multi-agentbidding solution (DCMAB) has archieved promising results and thus thay plan to conduct a live test in Taobao platform.\n",
    "\n",
    "The details of the implementation are left to users to investigate. Generally apeaking, Taobao ad platform is a place for merchants to place a bid in order to display at to the customers. This could be a multi-agent problem because the merchants are bidding against each other and their actions are interrelated. In the paper, merchants and customers were clusters into different groups to reduce computational complexicty. The stae space of the agents indicated the cost-revenue status of the agents, action space was the bid (continuous), and reward was the revenue caused by the customer cluster.\n",
    "\n",
    "___\n",
    "***DCMAB* Algorithm**, [link](https://arxiv.org/pdf/1802.09756.pdf)\n",
    "___\n",
    "- 1. Initialize $Q_i(s^q, a_1^q, ..., a_N^q, d\\mid\\theta^Q_i)$, actor $\\mu_i[g,x]\\mid\\theta^\\mu_i$ and target network $Q_i', \\mu_i'$ with the weights $\\theta_i^{Q'}\\leftarrow\\theta_i^{Q}, \\theta_i^{\\mu'}\\leftarrow\\theta^\\mu_i$ for agent $i$\n",
    "- 2. Initialize replay memory D\n",
    "- 3. **for** *episode* = 1 to $E$ **do**:<br>\n",
    "    - 3.1. Initialize a random process $\\mathcal{N}$ for action exploration<br>\n",
    "    - 3.2. Receive initial state $s$ for all agents<br>\n",
    "    - 3.3. **for** $t$ = 1 to $T$ **do**:<br>\n",
    "        - 3.3.1. For each agent $i$, compute $a_i^q$ and add $\\mathcal{N}_t$.<br>\n",
    "        - 3.3.2. **for** *auctions in parallel workers in $T_p$* **do**:<br>\n",
    "            - 3.3.2.1. For each agent $i$, compute *bratio* and combined with $a_i^q$ compute adjusting ratio $\\alpha$ and execute.<br>\n",
    "            - 3.3.2.2. For each agent $i$, save reward, cost and maintain distribution $d$.<br>\n",
    "        - 3.3.2'. **endfor**\n",
    "        - 3.3.3. For each agent $i$, merger rewards, cost in last $T_p$ to get reward $r_i$ and update state to $s^{q'}$. Store $(s^q, d, a^q_i, r_i, s^{q'})$ to replay memory\n",
    "        - 3.3.4. $s^{q'}\\leftarrow s^q$\n",
    "        - 3.3.5. **for** agent $i=1$ to $N$ **do**:\n",
    "            - 3.3.5.1. Sample a random minibatch of $S$ samples $(s^q, d, a^q_i, ..., a_N^q, r_i, s^{q'}, d')$ from $D$\n",
    "            - 3.3.5.2. Update critice by minimizing loss\n",
    "            - 3.3.5.3. Update actor\n",
    "            - 3.3.5.4. Update target network $\\theta'\\leftarrow\\tau\\theta+(1-\\tau)\\theta$\n",
    "        - 3.3.5'. **endfor**\n",
    "    - 3.3'. **endfor**\n",
    "- 3'. **endfor**\n",
    "___\n",
    "\n",
    "Other questions, including the impact of different reward settings (self-interested vs coordinated) on agents' revenue were also studied in the paper.\n",
    "\n",
    "### 2.8 Games\n",
    "RL is so well-known these day because it is the mainstream algorithm used to solve different games and sometimes archieve super-humand performance.\n",
    "![](https://cdn-images-1.medium.com/max/1200/1*74w_bGOcWY9h35PvIj2rSA.png)\n",
    "RL vs linear model vs Human, [source](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf)\n",
    "\n",
    "The most famous one must be AlphaGo [12] and AlphaGo Zero [13]. AlphaGo, trained with conutless human games, already archieved super-human performance by using value network and Monte Carlo tree search (MCTS) in its policy network. Yet, the researchers later on thought back tried a pureer RL approach -- train it from scratch. The researchers let the new agent, AlphaGo Zero, played with itself and finally bet AlphaGo 100-0.\n",
    "\n",
    "### 2.9 Deep Larning\n",
    "More and moer attepms to combine RL and other deep learning architecture can be seen recently and they showed impreeesive results. \n",
    "\n",
    "One of the most influential work in RL is the pioneering work of DeepMind to combine CNN with RL [7]. By doing so, the agent has the ability to \"see\" the environment through high-dimensional sensory and then learn to interact with it.\n",
    "\n",
    "RL and RNN is another combinations people used to try new idea. RNN is a type of neural network that has \"memories\". Ehwn combined with RL, RNN gives the agents' ability to memorize things. For example. [8] combinef LSTM with RL to crate Deep Recurrent Q-Network (DRQN) to play Atari 2600 games. [4] also used RNN and RL to solve chemical reaction optimization problem. \n",
    "\n",
    "Deepmind showed [9] how to use generative models and RL to generat programs. In the model, the adversarially trained agent used the signal as rewards to improve the actions, instead of propagating the gradients to the input space as in the GAN training.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1200/1*6K3oi5o09szmG6KEXxctvw.png)\n",
    "Input vs Generated results, [source](https://www.youtube.com/watch?v=N5oZIO8pE40)\n",
    "\n",
    "**( * -- ( * ) -- )**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.  What you need to know berofe applying RL to your problem\n",
    "\n",
    "There are several things needed before RL can be applied:\n",
    "- Understanding your problem: You do not necessarilly need to use RL in your problem and sometimes you just cannot use RL. You may want to check if your problem has soem of the following characteristics before deciding to use RL:\n",
    "    - a) trial-and-error (can be learned to do better by receiving feddback from the environment);\n",
    "    - b) delayed rewards;\n",
    "    - c) can be modeled as MDP;\n",
    "    - d) your problem is a control problem\n",
    "- A simulated environment: Lots of iterations are needed before a RL algortihm to wrok. I am sure that you don't want to see a RL agetn trying different things in a self-driving car on a high way, right? Therefore, a simulated environment that can correctly reflect the real world is needed.\n",
    "- MDP: You would need to formulate your problem into a MDP. You need to design the state space, action space, reward function and so on. Your agent will do what it is rewarded to to under the constraints. You may not get the results you want if you design the things differently.\n",
    "- Algorithm: There are different RL algorithms you can choose and questions to ask yourself. You want to directly find out the policy or you want to learn the value function? You want to go model-free or model-based? Do you need to combine other kinds of deep neural network or methods to solve your problem?\n",
    "\n",
    "To stay objective and fais, you are also warned about the shortcoming of RL and [here is a great post](https://www.alexirpan.com/2018/02/14/rl-hard.html) about it.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Intuitions from other disciplines\n",
    "\n",
    "RL has a very close relationship with psychology, biology ans neuroscience. If you think about it, waht a RL agent does is just trial-and-error: its learns how good or bad its action are based on the rewards it receives from the environment. And [this is exactly how human learn to make a decision](http://www.princeton.edu/~yael/ICMLTutorial.pdf). Besides, the exploration and exploitation problem, credit assignment problem, attemps to model the environment are also something we face in our everyday life.\n",
    "\n",
    "The Economics theory can also shed some light on RL. In particaular, the analysis of multi-agent reinforcement learning (MARL) can be understood from the prespectives of game theory, which is a research area developed by John Nash to understand the interactions of agent in a system. In addition to game theory, MARL< Partially Observable Markov Decision Process (POMDP) could also be useful to understand other economic topics like [market structure](https://en.wikipedia.org/wiki/Market_structure) (e.g. monopoly, oliopoly, etc.) [externality](https://en.wikipedia.org/wiki/Externality) and [information asymmetry](https://en.wikipedia.org/wiki/Information_asymmetry)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. What could RL possibly achieve in the future\n",
    "\n",
    "RL still ha lots of problems and cannot be used easily. Yet, as long as more efforts are put in solving the problems, RL would be influential and impactful in the following ways:\n",
    "- Assiting human:May be it is too mucj to say RL can one day evolve into artificial general intelligence (AGI), but RL surely has the potential to assist and work with human. Just imagine a robot or a virtual assistant working with you and taking your actions into its considerations to tak anctions in order to achieve a common goal. Wouldn't be right?\n",
    "- Understanding the consequences of different strategies: Life is so amazing because time will not go back and things just happen once. Yet, sometimes we would like to know things could be different (at least in the short term) if I tool a different actions? Or would Crotias has a greater chance to win the 2018 World Cup if the coach used another strategy? Of course, to achieve this we would need to model the environment, transition functions, and so on perfectly and also analyse the environments between the agents, which seems to be impossible at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Conclusion\n",
    "\n",
    "This article just showed some of the examples of RL applications in various industries. They should not limit your RL use case and as always, you should use the first principle to understand the nature of RL and your problem.\n",
    "\n",
    "If you are a decision maker of a company, I hope this article is enough to persuade you to rethink about your business and see if RL can be potentially used. If you are a researcher, I hope you would agree with me that although RL still has different shortcomings, its menas it has lost of potentials to improve and lots of research opportunities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII. References\n",
    "[1]G. Zheng, F. Zhang, Z. Zheng, Y. Xiang, Ni. J. Yuan, X. Xie, and Z. Li. DRN: A Deep Reinforcement Learning Frameworkfor News Recommendation. 2018.\n",
    "\n",
    "[2] H.Mao, Alizadeh, M. Alizadeh, Menache, I.Menache, and S.Kandula. Resource Management With deep Reinforcement Learning. In ACM Workshop on Hot Topics in Networks, 2016.\n",
    "\n",
    "[3] I. Arel, C. Liu, T. Urbanik, and A. Kohls, “Reinforcement learning-based multi-agent system for network traffic signal control,”IET IntelligentTransport Systems, 2010.\n",
    "\n",
    "[4] Z. Zhou, X. Li, and R. N. Zare. Optimizing Chemical Reactions with Deep Reinforcement Learning. ACSCentral Science3, 2017.\n",
    "\n",
    "[5] X. Bu, J. Rao, C. Z. Xu. A reinforcement learning approach to online web systems auto-configuration. In Distributed Computing Systems, 2009. ICDCS’09.29th IEEE International Conference on. IEEE , 2019.\n",
    "\n",
    "[6]J. Jin, C.Song, H. Li, K. Gai, J.Wang amd W. Zhang. Real-Time Bidding with Multi-Agent Reinforcement Learningin Display Advertising. arXiv preprint arXiv:1802.09756, 2018.\n",
    "\n",
    "[7] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.\n",
    "\n",
    "[8] M. J. Hausknecht and P. Stone. Deep Recurrent Q-Learning For Partially Observable MDPs. Proc. of Conf. on Artificial Intelligence, AAAI, 2015.\n",
    "\n",
    "[9] Y. Ganin, T. Kulkarni, I. Babuschkin, S. Eslami and O. Vinyals. Synthesizing Programs For Images Using Reinforced Adversarial Learning. arXiv preprintarXiv:1804.01118.\n",
    "\n",
    "[10] J. Kober, J. A. D. Bagnell, J. Peters. Reinforcement Learning in Robotics: A survey. Int. J. Robot. Res. Jul. 2013.\n",
    "\n",
    "[11] S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end Training of Deep Visuomotor Policies. arXiv preprint arXiv:1504.00702, 2015.\n",
    "\n",
    "[12] D. Silver, A. Huang, A., C.J. Maddison, A. Guez, L. Sifre,G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Has-sabis. Mastering the game of go with deep neuralnetworks and tree search.Nature, 529(7587). 2016.\n",
    "\n",
    "[13] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen,T. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis. Mastering the game of go without human knowledge.Nature, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
