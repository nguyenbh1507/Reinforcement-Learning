{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convergence of Q-learning \n",
    "## Markov decision process (MDP): $(\\mathcal{X, A, P}, r)$\n",
    "\n",
    "where: \n",
    "- $\\mathcal{X}$ - state space\n",
    "- $\\mathcal{A}$ - action space\n",
    "- $\\mathcal{P}$ - transition probabilities\n",
    "- $r$ - reward function:\n",
    "$$r:\\mathcal(X,A,X)\\rightarrow\\mathbb{R}$$\n",
    "$$r(x,a,y) \\mapsto r$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**value of a state $x$ for a sequence of control $\\{\\mathcal{A}_t\\}$**:\n",
    "$$J(x,\\{\\mathcal{A}_t\\})=\\mathbb{E}\\left[\\sum_{t=0}^{\\infty}\\gamma^t\\mathcal{R}\\left(\\mathcal{X}_t, \\mathcal{A}_t\\right)\\mid \\mathcal{X}_0=x\\right]$$\n",
    "**optimal value function** $x\\in\\mathcal{X}$:\n",
    "$$V^*(x)=\\max_{\\mathcal{A}_t}J(x,\\{\\mathcal{A}_t\\})$$\n",
    "**optimal $Q^*$-function**:\n",
    "$$Q^*(x,a)=\\sum_{y\\in\\mathcal{X}}\\mathcal{P}_a(x,y)\\left[r(x,a,y)+\\gamma V^*(y)\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**optimal $Q$ function** is a fixed point of a contraction operator $\\mathbf{H}$:\n",
    "$$q:\\mathcal{X\\times A}\\rightarrow\\mathbb{R}$$\n",
    "$$(\\mathbf{H}q)(x,a)=\\sum_{y\\in\\mathcal{X}}\\mathcal{P}_a(x,y)\\left[r(x,a,y)+\\gamma\\max_{b\\in\\mathcal{A}}q(y,b)\\right]$$\n",
    "$$\\left \\| \\mathbf{H}q_1-\\mathbf{H}q_2 \\right \\|_\\infty\\leq\\gamma\\left \\| q_1-q_2 \\right \\|_\\infty \\tag{*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**for $(*)$**:\n",
    "$\\left \\| \\mathbf{H}q_1-\\mathbf{H}q_2 \\right \\|_\\infty $:\n",
    "\n",
    "$\\displaystyle =\\max_{x,a}\\left\\| \\sum_{r\\in\\mathcal{X}}\\mathcal{P}_a(x,y)\\left[ r(x,a,y)+\\gamma\\max_{b\\in\\mathcal{A}}q_1(y,b) \\right] - \\sum_{r\\in\\mathcal{X}}\\mathcal{P}_a(x,y)\\left[ r(x,a,y)+\\gamma\\max_{b\\in\\mathcal{A}}q_2(y,b) \\right] \\right\\|$\n",
    "\n",
    "$\\displaystyle=\\max_{x,a}\\gamma \\left | \\sum_{r\\in\\mathcal{X}}\\mathcal{P}_a(x,y) \\left[ \\max_{b\\in\\mathcal{A}}q_1(y,b) - \\max_{b\\in\\mathcal{A}}q_1(y,b) \\right] \\right |  $ (because of $|a+b|\\leq|a|+|b|$)\n",
    "\n",
    "$\\displaystyle \\leq \\max_{x,a}\\gamma \\sum_{r\\in\\mathcal{X}}\\mathcal{P}_a(x,y) \\left| \\max_{b\\in\\mathcal{A}}q_1(y,b) - \\max_{b\\in\\mathcal{A}}q_1(y,b) \\right|$ (because of $|\\max A - \\max B|\\leq\\max|A-B|$)\n",
    "\n",
    "$\\displaystyle \\leq \\max_{x,a}\\gamma \\sum_{r\\in\\mathcal{X}}\\mathcal{P}_a(x,y) \\max_{z\\in\\mathcal{X},\\ b\\in\\mathcal{A}}\\left| q_1(z,b)-q_2(z,b) \\right|$\n",
    "\n",
    "$\\displaystyle = \\max_{x,a}\\gamma \\sum_{r\\in\\mathcal{X}}\\mathcal{P}_a(x,y) \\left\\| q_1-q_2 \\right\\|_\\infty$\n",
    "\n",
    "$=\\gamma\\left\\| q_1-q_2 \\right\\|_\\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning algorithm determines the optimal q-funciton using point sample:\n",
    "<br>Assumed $\\pi$ is a random policy. such that:\n",
    "$$\\mathcal{P}_\\pi\\left[ \\mathcal{A}_t=a\\mid\\mathcal{X}_t=x\\right]>0$$\n",
    "i.e. probability run action $a$ while the state is $x$ over policy $\\pi$ is possitive\n",
    "\n",
    "For all pairs $(x, a)$, where:\n",
    "\n",
    "-$\\{x_t\\}$: state sequence following $\\pi$\n",
    "-$\\{a_t\\}$: correspoding action sequence\n",
    "-$\\{r_t\\}$: correspoding reward sequnece\n",
    "\n",
    "then given any initial estimation $Q_0$, Q-learning updating:\n",
    "\n",
    "$$Q_{t+1}(x_t,a_t) = Q_t(x_t, a_t)+\\alpha_t(x_t,a_t)\\left[r_t+\\gamma\\max_{b}Q_t(x_{t+1}, b)-Q_t(x_t,a_t) \\right]\\tag{**}$$\n",
    "where:\n",
    "- $Q_t(x_t, a_t)$: estimation for action $a_t$ at an state $x_t$ and time step $t$\n",
    "- $\\alpha_t(x_t,a_t)\\in[0,1)$: learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Theorem**:\n",
    "\n",
    "Given MDP $(\\mathcal{X, A,P}, r)$, **Q-learning** algorithm, describe as $(**)$ converges with probability 1 (w.p.1) if:\n",
    "\n",
    "$$\\sum_t \\alpha_t(x_t, a_t) = \\infty\\quad\\text{and}\\quad\\sum_t \\alpha_t^2(x_t, a_t) < \\infty \\tag{***}$$\n",
    "\n",
    "Since $0\\leq\\alpha_t(x,a)<1$ $(***)$ requires that all pairs $(x,a)$ be visited infinitely often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above theorem is proved by following theorem:\n",
    "## Theorem 2:\n",
    "Random process $\\{\\Delta_t\\}$ taking values in $\\mathbb{R}^n$ and defined as:\n",
    "$$\\Delta_{t+1}(x)=[1-\\alpha_t(x)][\\Delta_t(x)+\\alpha_t(x)F_t(x)]$$\n",
    "cpnverges to zeros with probability 1 if:\n",
    "- $0\\leq\\alpha_t<1;\\quad\\sum_{t}\\alpha_t=\\infty,\\quad \\sum_{t}\\alpha_t^2<\\infty$\n",
    "- $\\left\\|\\mathbb{E}\\left[F_t(x)\\mid\\mathcal{F}\\right] \\right\\|_W\\leq\\gamma\\left\\|\\Delta_t\\right\\|_W$\n",
    "- $\\mathbf{var}\\left[F_t(x)\\mid\\mathcal{F}\\right]\\leq C(1+\\left\\|\\Delta_t\\right\\|_W^2),\\text{ for} C>0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof:\n",
    "\n",
    "We have:\n",
    "$$Q_{t+1}(x_t, a_t) = [1-\\alpha_t(x_t, a_t)]Q_{t}(x_t, a_t)\\quad+\\quad\\alpha_t(x_t,a_t)[r_t+\\gamma\\max_{b\\in\\mathcal{A}}Q_t(x_{t+1},b)]$$\n",
    "$$Q^*(x_t, a_t)=\\sum_{y\\in\\mathcal{X}}\\mathcal{P}_{a_t}(x_t,y)\\left[r(x_t, a_t, y)+\\gamma\\max_{b\\in\\mathcal{A}}Q^*(x_t,b)\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q_{t+1}(x_t, a_t)-Q^*(x_t, a_t)=Q_{t+1}(x_t, a_t)-[1-\\alpha_t(x_t, a_t)]Q^*(x_t, a_t)-\\alpha_t(x_t, a_t)Q^*(x_t, a_t)$\n",
    "\n",
    "= $\\displaystyle(1-\\alpha_t(x_t, a_t))[Q_{t}(x_t, a_t)-Q^*(x_t, a_t)]\\quad+\\quad\\alpha_t(x_t,a_t)[r_t+\\gamma\\max_{b\\in\\mathcal{A}}Q_t(x_{t+1},b)-Q^*(x_t, a_t)]$\n",
    "\n",
    "=$\\displaystyle(1-\\alpha_t(x_t, a_t))\\Delta_t(x_t,a_t)\\quad+\\quad\\alpha_t(x_t,a_t)[r_t+\\gamma\\max_{b\\in\\mathcal{A}}Q_t(x_{t+1},b)-Q^*(x_t, a_t)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\n",
    "$$F_t(x,a) =r\\left(x,a,X(x,a)\\right)+\\gamma\\max_{b\\in\\mathcal{A}}Q_t(y,b)-Q^*(x,a)$$\n",
    "where $X(x,a)$ is random sample state, obtained from $(\\mathcal{X,P_a})$ then:\n",
    "\n",
    "$$\\mathbb{E}\\left[F_t(x,a)\\mid\\mathcal{F_t}\\right]=\\sum_{y\\in\\mathcal{X}}\\mathcal{P_a}(x,y)\\left[r(x,a,y)+\\gamma\\max_{b\\in\\mathcal{A}}Q_t(y,b)-Q^*(x,a)\\right] \n",
    "\\\\= \\sum_{y\\in\\mathcal{X}}\\mathcal{P_a}(x,y)\\left[r(x,a,y)+\\gamma\\max_{b\\in\\mathcal{A}}Q_t(y,b)\\right]-   \\sum_{y\\in\\mathcal{X}}\\mathcal{P_a}(x,y)\\left[Q^*(x,a)\\right]\n",
    "\\\\=\\mathbf{H}Q_t-Q^*(x,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the fact that $\\mathbf{H}Q^*=Q^*$:\n",
    "\n",
    "$$\\mathbb{E}\\left[F_t(x,a)\\mid\\mathcal{F_t}\\right]=\\mathbf{H}Q_t(x,a)-\\mathbf{H}Q^*(x,a)$$\n",
    "\n",
    "$$\\mathbb{E}\\left[F_t(x,a)\\mid\\mathcal{F_t}\\right]\\leq\\gamma\\left\\|Q_t-Q^*\\right|_\\infty=\\gamma\\left\\|\\Delta_t\\right\\|_\\infty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally**:\n",
    "\n",
    "$\\mathbf{var}\\left[F_t(x,a)\\mid\\mathcal{F_t}\\right]$\n",
    "\n",
    "$=\\mathbb{E}\\left[\\left(F_t(x,a)-\\mathbb{E}\\left[F_t(x,a)\\mid\\mathcal{F_t}\\right]\\right)^2\\right]$\n",
    "\n",
    "$=\\mathbb{E}\\left[\\left(r(x,a,X(x,a))+\\gamma\\max_{b\\in\\mathcal{A}}Q_t(y,b)-Q^*(x,a)-\\mathbf{H}Q_t(x,a)+Q^*(x,a) \\right)^2\\right]$\n",
    "\n",
    "Because: $\\displaystyle\\quad\\mathbf{H}Q_t(x,a)=\\sum_{y\\in\\mathcal{X}}\\mathcal{P_a}(x,y)\\left[ r(x,a,y) +\\gamma\\max_{b\\in\\mathcal{A}}Q_t(y,b)\\right] =\\mathbb{E}\\left[ r(x,a,y) +\\gamma\\max_{b\\in\\mathcal{A}}Q_t(y,b) \\right]$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\\mathbf{var}\\left[F_t(x,a)\\mid\\mathcal{F_t}\\right]= \\mathbf{var}\\left[ r(x,a,y) +\\gamma\\max_{b\\in\\mathcal{A}}Q_t(y,b) \\mid\\mathcal{F_t}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which, due to the fact that $r$ is bounded, clearly verifies:\n",
    "\n",
    "$$\\mathbf{var}\\left[F_t(x,a)\\mid\\mathcal{F_t}\\right]\\leq C(1+\\|\\Delta_t\\|_W^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then by **theorem 2** \n",
    "\n",
    "$\\Delta_t=Q_t-Q^*_t$ converges to $0$\n",
    "\n",
    "i.e. $Q_t$ converges to $Q^*_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
